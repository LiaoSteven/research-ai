#!/usr/bin/env python3
"""
é‡‡é›† AI vs éAI è§†é¢‘è¯„è®ºè¿›è¡Œå¯¹æ¯”ç ”ç©¶

è‡ªåŠ¨æ£€æµ‹è§†é¢‘æ˜¯å¦åŒ…å« AI å†…å®¹ï¼Œå¹¶é‡‡é›†ä¸¤ç»„æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ

ä½¿ç”¨æ–¹æ³•ï¼š
    python collect_ai_comparison.py --ai-videos 500 --non-ai-videos 500
"""

import sys
from pathlib import Path
import argparse
import os
import json
import time
from datetime import datetime

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src" / "main" / "python"))

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å¯¼å…¥æ¨¡å—
import services.youtube_collector as yt_module
YouTubeCollector = yt_module.YouTubeCollector

# å¯¼å…¥ AI æ£€æµ‹å™¨
from detect_ai_content import AIContentDetector


class ComparisonCollector:
    """é‡‡é›† AI vs éAI è§†é¢‘è¯„è®ºç”¨äºå¯¹æ¯”ç ”ç©¶"""

    # AI ç›¸å…³æœç´¢å…³é”®è¯
    AI_SEARCH_QUERIES = [
        'AI generated shorts',
        'AI art shorts',
        'AI animation shorts',
        'midjourney shorts',
        'stable diffusion shorts',
        'AI video shorts',
        'generative AI shorts'
    ]

    # é AI æœç´¢å…³é”®è¯
    NON_AI_SEARCH_QUERIES = [
        'handmade shorts',
        'traditional art shorts',
        'real footage shorts',
        'vlog shorts',
        'cooking shorts',
        'DIY shorts',
        'tutorial shorts'
    ]

    def __init__(self, api_key: str):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.collector = YouTubeCollector(api_key=api_key)
        self.detector = AIContentDetector(api_key=api_key)
        self.youtube = self.collector.youtube

    def search_videos(
        self,
        queries: list,
        max_results: int = 50,
        region: str = 'US'
    ) -> list:
        """
        æœç´¢è§†é¢‘

        Args:
            queries: æœç´¢å…³é”®è¯åˆ—è¡¨
            max_results: æœ€å¤šè¿”å›å¤šå°‘ä¸ªè§†é¢‘
            region: åœ°åŒºä»£ç 

        Returns:
            è§†é¢‘ ID åˆ—è¡¨
        """
        video_ids = []
        found_videos = set()

        print(f"\nğŸ” æœç´¢è§†é¢‘...")
        print(f"   å…³é”®è¯æ•°é‡: {len(queries)}")

        for query in queries:
            if len(video_ids) >= max_results:
                break

            print(f"   æœç´¢: '{query}'")

            try:
                search_params = {
                    'part': 'id,snippet',
                    'type': 'video',
                    'q': query,
                    'videoDuration': 'short',
                    'maxResults': min(10, max_results - len(video_ids)),
                    'order': 'relevance',
                    'regionCode': region
                }

                request = self.youtube.search().list(**search_params)
                response = request.execute()

                for item in response.get('items', []):
                    if 'videoId' in item['id']:
                        video_id = item['id']['videoId']
                        if video_id not in found_videos:
                            found_videos.add(video_id)
                            video_ids.append(video_id)
                            print(f"      âœ“ {video_id}")

                time.sleep(1)

            except Exception as e:
                print(f"      âœ— é”™è¯¯: {e}")
                continue

        print(f"âœ… æ‰¾åˆ° {len(video_ids)} ä¸ªè§†é¢‘")
        return video_ids

    def collect_with_detection(
        self,
        target_type: str,  # 'ai' or 'non_ai'
        max_comments: int = 500,
        per_video: int = 50,
        region: str = 'US',
        verify_threshold: float = 0.3
    ) -> tuple:
        """
        é‡‡é›†å¹¶éªŒè¯è§†é¢‘ç±»å‹

        Args:
            target_type: 'ai' æˆ– 'non_ai'
            max_comments: ç›®æ ‡è¯„è®ºæ•°
            per_video: æ¯è§†é¢‘è¯„è®ºæ•°
            region: åœ°åŒºä»£ç 
            verify_threshold: AI æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            (comments, video_info_list)
        """
        print(f"\n{'='*70}")
        print(f" é‡‡é›† {target_type.upper()} å†…å®¹è§†é¢‘è¯„è®º")
        print(f"{'='*70}")

        # é€‰æ‹©æœç´¢å…³é”®è¯
        if target_type == 'ai':
            queries = self.AI_SEARCH_QUERIES
            label = 'ai_generated'
        else:
            queries = self.NON_AI_SEARCH_QUERIES
            label = 'non_ai'

        # è®¡ç®—éœ€è¦çš„è§†é¢‘æ•°
        videos_needed = (max_comments // per_video) * 2  # å¤šæœç´¢ä¸€äº›å¤‡ç”¨

        # æœç´¢è§†é¢‘
        video_ids = self.search_videos(queries, videos_needed, region)

        if not video_ids:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘")
            return [], []

        # éªŒè¯å’Œè¿‡æ»¤
        print(f"\nğŸ” éªŒè¯è§†é¢‘å†…å®¹...")
        verified_videos = []

        for video_id in video_ids:
            result = self.detector.detect_ai_content(video_id, verbose=False)

            # åˆ¤æ–­æ˜¯å¦ç¬¦åˆç›®æ ‡ç±»å‹
            if target_type == 'ai':
                # éœ€è¦æ˜¯ AI å†…å®¹
                if result['confidence'] >= verify_threshold:
                    verified_videos.append(result)
                    print(f"   âœ“ {video_id}: AI å†…å®¹ (ç½®ä¿¡åº¦ {result['confidence']:.2f})")
            else:
                # éœ€è¦ä¸æ˜¯ AI å†…å®¹
                if result['confidence'] < verify_threshold:
                    verified_videos.append(result)
                    print(f"   âœ“ {video_id}: é AI å†…å®¹ (ç½®ä¿¡åº¦ {result['confidence']:.2f})")

            if len(verified_videos) >= (max_comments // per_video):
                break

        print(f"\nâœ… éªŒè¯é€šè¿‡ {len(verified_videos)} ä¸ªè§†é¢‘")

        if not verified_videos:
            print("âŒ æ²¡æœ‰é€šè¿‡éªŒè¯çš„è§†é¢‘")
            return [], []

        # é‡‡é›†è¯„è®º
        print(f"\nğŸ“ å¼€å§‹é‡‡é›†è¯„è®º...")
        all_comments = []
        video_info_list = []

        for idx, result in enumerate(verified_videos, 1):
            if len(all_comments) >= max_comments:
                print(f"\nâœ… å·²è¾¾åˆ°ç›®æ ‡è¯„è®ºæ•° {max_comments}")
                break

            video_id = result['video_id']
            print(f"\n[{idx}/{len(verified_videos)}] å¤„ç†: {video_id}")
            print(f"  æ ‡é¢˜: {result['title'][:50]}...")
            print(f"  AI ç½®ä¿¡åº¦: {result['confidence']:.2f}")

            try:
                # è·å–è§†é¢‘ä¿¡æ¯
                video_info = self.collector.get_video_info(video_id)
                video_info['video_type'] = label
                video_info['ai_confidence'] = result['confidence']
                video_info['ai_indicators'] = result['indicators']
                video_info_list.append(video_info)

                # è·å–è¯„è®º
                comments = self.collector.get_video_comments(
                    video_id,
                    max_comments=per_video,
                    include_replies=True
                )

                # æ·»åŠ æ ‡ç­¾
                for comment in comments:
                    comment['video_type'] = label
                    comment['ai_confidence'] = result['confidence']

                all_comments.extend(comments)
                print(f"  âœ“ é‡‡é›† {len(comments)} æ¡è¯„è®º (æ€»è®¡: {len(all_comments)})")

                time.sleep(1)

            except Exception as e:
                print(f"  âœ— å¤±è´¥: {e}")
                continue

        return all_comments, video_info_list


def main():
    parser = argparse.ArgumentParser(description='é‡‡é›† AI vs éAI è§†é¢‘è¯„è®ºè¿›è¡Œå¯¹æ¯”ç ”ç©¶')
    parser.add_argument('--ai-comments', type=int, default=500,
                       help='AI å†…å®¹è¯„è®ºæ•°ï¼ˆé»˜è®¤ 500ï¼‰')
    parser.add_argument('--non-ai-comments', type=int, default=500,
                       help='é AI å†…å®¹è¯„è®ºæ•°ï¼ˆé»˜è®¤ 500ï¼‰')
    parser.add_argument('--per-video', type=int, default=50,
                       help='æ¯è§†é¢‘è¯„è®ºæ•°ï¼ˆé»˜è®¤ 50ï¼‰')
    parser.add_argument('--region', type=str, default='US',
                       help='åœ°åŒºä»£ç ï¼ˆé»˜è®¤ USï¼‰')
    parser.add_argument('--threshold', type=float, default=0.3,
                       help='AI æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.3ï¼‰')

    args = parser.parse_args()

    print("\n" + "="*70)
    print(" AI vs éAI è§†é¢‘è¯„è®ºå¯¹æ¯”é‡‡é›†å·¥å…·")
    print("="*70)

    # æ£€æŸ¥ API key
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° YouTube API å¯†é’¥")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® YOUTUBE_API_KEY")
        return 1

    # åˆå§‹åŒ–é‡‡é›†å™¨
    try:
        collector = ComparisonCollector(api_key=api_key)
        print("\nâœ… YouTube API è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1

    print(f"\nğŸ“Š é‡‡é›†å‚æ•°:")
    print(f"  AI å†…å®¹è¯„è®º: {args.ai_comments}")
    print(f"  é AI è¯„è®º: {args.non_ai_comments}")
    print(f"  æ¯è§†é¢‘è¯„è®º: {args.per_video}")
    print(f"  æ£€æµ‹é˜ˆå€¼: {args.threshold}")
    print(f"  åœ°åŒº: {args.region}")

    # 1. é‡‡é›† AI å†…å®¹è¯„è®º
    ai_comments, ai_videos = collector.collect_with_detection(
        target_type='ai',
        max_comments=args.ai_comments,
        per_video=args.per_video,
        region=args.region,
        verify_threshold=args.threshold
    )

    # 2. é‡‡é›†é AI å†…å®¹è¯„è®º
    non_ai_comments, non_ai_videos = collector.collect_with_detection(
        target_type='non_ai',
        max_comments=args.non_ai_comments,
        per_video=args.per_video,
        region=args.region,
        verify_threshold=args.threshold
    )

    # ä¿å­˜ç»“æœ
    output_dir = Path('data/raw')
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if ai_comments:
        ai_comments_file = output_dir / f'comments_ai_generated_{timestamp}.json'
        ai_videos_file = output_dir / f'videos_ai_generated_{timestamp}.json'

        with open(ai_comments_file, 'w', encoding='utf-8') as f:
            json.dump(ai_comments, f, ensure_ascii=False, indent=2)
        with open(ai_videos_file, 'w', encoding='utf-8') as f:
            json.dump(ai_videos, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ AI å†…å®¹æ•°æ®å·²ä¿å­˜:")
        print(f"  è¯„è®º: {ai_comments_file}")
        print(f"  è§†é¢‘: {ai_videos_file}")

    if non_ai_comments:
        non_ai_comments_file = output_dir / f'comments_non_ai_{timestamp}.json'
        non_ai_videos_file = output_dir / f'videos_non_ai_{timestamp}.json'

        with open(non_ai_comments_file, 'w', encoding='utf-8') as f:
            json.dump(non_ai_comments, f, ensure_ascii=False, indent=2)
        with open(non_ai_videos_file, 'w', encoding='utf-8') as f:
            json.dump(non_ai_videos, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ é AI å†…å®¹æ•°æ®å·²ä¿å­˜:")
        print(f"  è¯„è®º: {non_ai_comments_file}")
        print(f"  è§†é¢‘: {non_ai_videos_file}")

    # ç»Ÿè®¡ç»“æœ
    print("\n" + "="*70)
    print(" é‡‡é›†å®Œæˆï¼")
    print("="*70)
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  AI å†…å®¹è¯„è®º: {len(ai_comments)} æ¡ (æ¥è‡ª {len(ai_videos)} ä¸ªè§†é¢‘)")
    print(f"  é AI è¯„è®º: {len(non_ai_comments)} æ¡ (æ¥è‡ª {len(non_ai_videos)} ä¸ªè§†é¢‘)")
    print(f"  æ€»è®¡: {len(ai_comments) + len(non_ai_comments)} æ¡è¯„è®º")

    print(f"\nâœ¨ ä¸‹ä¸€æ­¥:")
    print(f"  1. é¢„å¤„ç†æ•°æ®:")
    if ai_comments:
        print(f"     python scripts/preprocess_data.py --input {ai_comments_file}")
    if non_ai_comments:
        print(f"     python scripts/preprocess_data.py --input {non_ai_comments_file}")
    print(f"  2. åˆå¹¶ä¸¤ç»„æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ")
    print(f"  3. è¿è¡Œæƒ…æ„Ÿåˆ†æå’Œä¸»é¢˜å»ºæ¨¡")
    print(f"  4. æ¯”è¾ƒ AI vs éAI çš„å·®å¼‚")

    print()
    return 0


if __name__ == '__main__':
    sys.exit(main())
