#!/usr/bin/env python3
"""
ç®€å•çš„æ•°æ®é‡‡é›†è„šæœ¬ - é‡‡é›† 1000 æ¡è¯„è®ºæ ·æœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   export YOUTUBE_API_KEY=your_api_key_here  # Linux/Mac
   set YOUTUBE_API_KEY=your_api_key_here     # Windows

2. åˆ›å»ºè§†é¢‘åˆ—è¡¨æ–‡ä»¶ video_urls.txtï¼Œæ¯è¡Œä¸€ä¸ª YouTube é“¾æ¥ï¼Œä¾‹å¦‚ï¼š
   https://youtube.com/shorts/abc123
   https://youtube.com/shorts/def456

3. è¿è¡Œï¼š
   python collect_sample.py
"""

import sys
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src" / "main" / "python"))

import os
import json
from datetime import datetime

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
import services.youtube_collector as yt_module
YouTubeCollector = yt_module.YouTubeCollector


def main():
    print("\n" + "="*70)
    print(" YouTube è¯„è®ºé‡‡é›†å·¥å…·")
    print("="*70 + "\n")

    # æ£€æŸ¥ API key
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° YouTube API å¯†é’¥")
        print("\nè¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
        print("  Linux/Mac: export YOUTUBE_API_KEY=your_api_key_here")
        print("  Windows:   set YOUTUBE_API_KEY=your_api_key_here")
        print("\næˆ–è€…åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® YOUTUBE_API_KEY=your_api_key_here")
        return 1

    # è¯»å–è§†é¢‘ URL
    video_file = 'video_urls.txt'
    if not Path(video_file).exists():
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° {video_file}")
        print(f"\nè¯·åˆ›å»º {video_file} æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ª YouTube è§†é¢‘é“¾æ¥ï¼Œä¾‹å¦‚ï¼š")
        print("  https://youtube.com/shorts/abc123")
        print("  https://youtube.com/shorts/def456")
        print("  https://youtu.be/xyz789")
        return 1

    with open(video_file, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    if not urls:
        print(f"âŒ é”™è¯¯ï¼š{video_file} æ–‡ä»¶æ˜¯ç©ºçš„")
        return 1

    print(f"ğŸ“‹ æ‰¾åˆ° {len(urls)} ä¸ªè§†é¢‘é“¾æ¥")

    # åˆå§‹åŒ–é‡‡é›†å™¨
    try:
        collector = YouTubeCollector(api_key=api_key)
        print("âœ… YouTube API è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1

    # æå–è§†é¢‘ ID
    video_ids = []
    for url in urls:
        video_id = collector.extract_video_id(url)
        if video_id:
            video_ids.append(video_id)
            print(f"  âœ“ è§†é¢‘ ID: {video_id}")
        else:
            print(f"  âœ— æ— æ³•è§£æ: {url}")

    if not video_ids:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘ ID")
        return 1

    print(f"\nğŸ“Š å‡†å¤‡é‡‡é›†è¯„è®º...")
    print(f"   ç›®æ ‡ï¼šæ¯ä¸ªè§†é¢‘ 100 æ¡è¯„è®º")
    print(f"   é¢„è®¡æ€»æ•°ï¼šçº¦ {len(video_ids) * 100} æ¡\n")

    # é‡‡é›†è¯„è®º
    all_comments = []
    video_info_list = []

    for idx, video_id in enumerate(video_ids, 1):
        print(f"\n[{idx}/{len(video_ids)}] å¤„ç†è§†é¢‘: {video_id}")

        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = collector.get_video_info(video_id)
            print(f"  æ ‡é¢˜: {video_info['title'][:50]}...")
            print(f"  è¯„è®ºæ•°: {video_info['comment_count']}")
            video_info_list.append(video_info)

            # è·å–è¯„è®º
            comments = collector.get_video_comments(
                video_id,
                max_comments=100,  # æ¯ä¸ªè§†é¢‘ 100 æ¡
                include_replies=True
            )

            all_comments.extend(comments)
            print(f"  âœ“ é‡‡é›†äº† {len(comments)} æ¡è¯„è®º")

            # å¦‚æœå·²ç»è¾¾åˆ° 1000 æ¡ï¼Œå°±åœæ­¢
            if len(all_comments) >= 1000:
                print(f"\nâœ… å·²é‡‡é›† {len(all_comments)} æ¡è¯„è®ºï¼Œè¾¾åˆ°ç›®æ ‡ï¼")
                break

        except Exception as e:
            print(f"  âœ— é‡‡é›†å¤±è´¥: {e}")
            continue

    # ä¿å­˜ç»“æœ
    if all_comments:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path('data/raw')
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comments_file = output_dir / f'comments_sample_{timestamp}.json'
        videos_file = output_dir / f'videos_info_{timestamp}.json'

        # ä¿å­˜è¯„è®º
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(all_comments, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è§†é¢‘ä¿¡æ¯
        with open(videos_file, 'w', encoding='utf-8') as f:
            json.dump(video_info_list, f, ensure_ascii=False, indent=2)

        print("\n" + "="*70)
        print(" é‡‡é›†å®Œæˆï¼")
        print("="*70)
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"  æ€»è¯„è®ºæ•°ï¼š{len(all_comments)}")
        print(f"  å¤„ç†è§†é¢‘ï¼š{len(video_info_list)}")
        print(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜ä½ç½®ï¼š")
        print(f"  è¯„è®ºæ•°æ®ï¼š{comments_file}")
        print(f"  è§†é¢‘ä¿¡æ¯ï¼š{videos_file}")

        # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
        print(f"\nğŸ“ è¯„è®ºæ ·æœ¬ï¼ˆå‰ 3 æ¡ï¼‰ï¼š")
        for i, comment in enumerate(all_comments[:3], 1):
            text = comment['text'][:60] + "..." if len(comment['text']) > 60 else comment['text']
            print(f"  {i}. {comment['author']}: {text}")

        print("\nâœ¨ ä¸‹ä¸€æ­¥ï¼š")
        print(f"  1. æŸ¥çœ‹æ•°æ®ï¼šcat {comments_file}")
        print(f"  2. é¢„å¤„ç†ï¼špython scripts/preprocess_data.py --input {comments_file} --output data/processed/comments.csv")
        print(f"  3. åˆ†æï¼šä½¿ç”¨ Jupyter Notebook æˆ– pandas è¿›è¡Œæ¢ç´¢æ€§åˆ†æ")
        print()

        return 0
    else:
        print("\nâŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•è¯„è®º")
        return 1


if __name__ == '__main__':
    sys.exit(main())
