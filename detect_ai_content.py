#!/usr/bin/env python3
"""
æ£€æµ‹ YouTube è§†é¢‘æ˜¯å¦åŒ…å« AI ç”Ÿæˆå†…å®¹

ä½¿ç”¨ YouTube Data API v3 æ£€æŸ¥è§†é¢‘çš„æè¿°ã€æ ‡ç­¾å’Œå…ƒæ•°æ®ï¼Œ
åˆ¤æ–­è§†é¢‘æ˜¯å¦å£°æ˜åŒ…å« AI/åˆæˆå†…å®¹

ä½¿ç”¨æ–¹æ³•ï¼š
    python detect_ai_content.py VIDEO_ID
    python detect_ai_content.py --file video_ids.txt
"""

import sys
import os
import argparse
from pathlib import Path

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError


class AIContentDetector:
    """æ£€æµ‹è§†é¢‘ä¸­çš„ AI ç”Ÿæˆå†…å®¹"""

    # YouTube å®˜æ–¹ AI æ ‡æ³¨å…³é”®è¯ï¼ˆå¤šè¯­è¨€ï¼‰
    AI_DISCLOSURE_KEYWORDS = {
        'chinese': [
            'æ­¤å½±ç‰‡åŒ…å«è®Šé€ æˆ–åˆæˆå…§å®¹',
            'æ­¤è§†é¢‘åŒ…å«å˜é€ æˆ–åˆæˆå†…å®¹',
            'AI ç”Ÿæˆ',
            'äººå·¥æ™ºèƒ½ç”Ÿæˆ',
            'åˆæˆå†…å®¹',
            'è®Šé€ å…§å®¹'
        ],
        'english': [
            'altered or synthetic content',
            'AI-generated',
            'AI generated',
            'synthetic content',
            'artificially generated',
            'created with AI',
            'made with AI',
            'AI-created'
        ],
        'spanish': [
            'contenido alterado o sintÃ©tico',
            'generado por IA',
            'contenido sintÃ©tico'
        ],
        'japanese': [
            'å¤‰æ›´ã¾ãŸã¯åˆæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„',
            'AIç”Ÿæˆ',
            'åˆæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„'
        ]
    }

    # å¸¸è§ AI å·¥å…·å’Œå¹³å°åç§°
    AI_TOOLS = [
        'midjourney',
        'stable diffusion',
        'dall-e',
        'dalle',
        'chatgpt',
        'gpt-4',
        'runway',
        'synthesia',
        'pictory',
        'invideo',
        'fliki',
        'lumen5',
        'descript',
        'wonder dynamics',
        'pika labs',
        'gen-2',
        'firefly',
        'adobe firefly',
        'bing image creator',
        'leonardo ai'
    ]

    # AI å†…å®¹ç›¸å…³æ ‡ç­¾
    AI_TAGS = [
        'ai',
        'artificial intelligence',
        'ai generated',
        'ai art',
        'generative ai',
        'machine learning',
        'neural network',
        'text to image',
        'text to video',
        'ai animation'
    ]

    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨

        Args:
            api_key: YouTube Data API v3 å¯†é’¥
        """
        self.youtube = build('youtube', 'v3', developerKey=api_key)

    def get_video_metadata(self, video_id: str) -> dict:
        """
        è·å–è§†é¢‘å®Œæ•´å…ƒæ•°æ®

        Args:
            video_id: YouTube è§†é¢‘ ID

        Returns:
            åŒ…å«è§†é¢‘å…ƒæ•°æ®çš„å­—å…¸
        """
        try:
            request = self.youtube.videos().list(
                part='snippet,contentDetails,statistics,status',
                id=video_id
            )
            response = request.execute()

            if not response.get('items'):
                return None

            video = response['items'][0]
            snippet = video['snippet']

            return {
                'video_id': video_id,
                'title': snippet.get('title', ''),
                'description': snippet.get('description', ''),
                'channel_title': snippet.get('channelTitle', ''),
                'tags': snippet.get('tags', []),
                'category_id': snippet.get('categoryId', ''),
                'published_at': snippet.get('publishedAt', ''),
                'thumbnail': snippet.get('thumbnails', {}).get('high', {}).get('url', ''),
                'view_count': video['statistics'].get('viewCount', 0),
                'like_count': video['statistics'].get('likeCount', 0),
                'comment_count': video['statistics'].get('commentCount', 0),
                'duration': video['contentDetails'].get('duration', ''),
                'made_for_kids': video['status'].get('madeForKids', False)
            }

        except HttpError as e:
            print(f"âŒ API é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            return None

    def detect_ai_content(self, video_id: str, verbose: bool = True) -> dict:
        """
        æ£€æµ‹è§†é¢‘æ˜¯å¦åŒ…å« AI ç”Ÿæˆå†…å®¹

        Args:
            video_id: YouTube è§†é¢‘ ID
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        metadata = self.get_video_metadata(video_id)

        if not metadata:
            return {
                'video_id': video_id,
                'is_ai_content': False,
                'confidence': 0.0,
                'error': 'Failed to fetch metadata'
            }

        title = metadata['title'].lower()
        description = metadata['description'].lower()
        tags = [tag.lower() for tag in metadata['tags']]

        # æ£€æµ‹ç»“æœ
        indicators = {
            'official_disclosure': False,
            'ai_tools_mentioned': [],
            'ai_keywords_in_title': [],
            'ai_keywords_in_description': [],
            'ai_tags': [],
            'confidence_score': 0.0
        }

        score = 0

        # 1. æ£€æŸ¥å®˜æ–¹ AI å†…å®¹å£°æ˜ï¼ˆæœ€é«˜æƒé‡ï¼‰
        for lang, keywords in self.AI_DISCLOSURE_KEYWORDS.items():
            for keyword in keywords:
                if keyword.lower() in description or keyword.lower() in title:
                    indicators['official_disclosure'] = True
                    score += 50  # å®˜æ–¹å£°æ˜æƒé‡éå¸¸é«˜
                    if verbose:
                        print(f"   âœ“ å‘ç°å®˜æ–¹å£°æ˜: '{keyword}' ({lang})")
                    break

        # 2. æ£€æŸ¥ AI å·¥å…·åç§°
        for tool in self.AI_TOOLS:
            if tool in title or tool in description:
                indicators['ai_tools_mentioned'].append(tool)
                score += 10
                if verbose:
                    print(f"   âœ“ å‘ç° AI å·¥å…·: {tool}")

        # 3. æ£€æŸ¥æ ‡é¢˜ä¸­çš„ AI å…³é”®è¯
        ai_keywords = ['ai', 'artificial intelligence', 'generated', 'synthetic']
        for keyword in ai_keywords:
            if keyword in title:
                indicators['ai_keywords_in_title'].append(keyword)
                score += 8  # æ ‡é¢˜æƒé‡è¾ƒé«˜
                if verbose:
                    print(f"   âœ“ æ ‡é¢˜åŒ…å«: {keyword}")

        # 4. æ£€æŸ¥æè¿°ä¸­çš„ AI å…³é”®è¯
        for keyword in ai_keywords:
            if keyword in description:
                indicators['ai_keywords_in_description'].append(keyword)
                score += 3
                if verbose:
                    print(f"   âœ“ æè¿°åŒ…å«: {keyword}")

        # 5. æ£€æŸ¥æ ‡ç­¾
        for tag in tags:
            if any(ai_tag in tag for ai_tag in self.AI_TAGS):
                indicators['ai_tags'].append(tag)
                score += 5
                if verbose:
                    print(f"   âœ“ AI ç›¸å…³æ ‡ç­¾: {tag}")

        # è®¡ç®—ç½®ä¿¡åº¦ (0-1)
        confidence = min(score / 100.0, 1.0)
        indicators['confidence_score'] = confidence

        # åˆ¤æ–­æ˜¯å¦ä¸º AI å†…å®¹ï¼ˆç½®ä¿¡åº¦ > 0.3ï¼‰
        is_ai_content = confidence >= 0.3

        result = {
            'video_id': video_id,
            'title': metadata['title'],
            'channel': metadata['channel_title'],
            'is_ai_content': is_ai_content,
            'confidence': confidence,
            'score': score,
            'indicators': indicators,
            'view_count': metadata['view_count'],
            'like_count': metadata['like_count'],
            'comment_count': metadata['comment_count']
        }

        return result

    def batch_detect(self, video_ids: list, verbose: bool = False) -> list:
        """
        æ‰¹é‡æ£€æµ‹å¤šä¸ªè§†é¢‘

        Args:
            video_ids: è§†é¢‘ ID åˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        results = []

        for i, video_id in enumerate(video_ids, 1):
            print(f"\n[{i}/{len(video_ids)}] æ£€æµ‹è§†é¢‘: {video_id}")
            result = self.detect_ai_content(video_id, verbose=verbose)
            results.append(result)

            if result['is_ai_content']:
                print(f"   ğŸ¤– AI å†…å®¹ (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            else:
                print(f"   ğŸ‘¤ é AI å†…å®¹ (ç½®ä¿¡åº¦: {result['confidence']:.2f})")

        return results


def main():
    parser = argparse.ArgumentParser(description='æ£€æµ‹ YouTube è§†é¢‘æ˜¯å¦åŒ…å« AI ç”Ÿæˆå†…å®¹')
    parser.add_argument('video_id', nargs='?', help='YouTube è§†é¢‘ ID')
    parser.add_argument('--file', help='åŒ…å«è§†é¢‘ ID çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    parser.add_argument('--output', '-o', help='ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶')

    args = parser.parse_args()

    if not args.video_id and not args.file:
        parser.print_help()
        return 1

    print("\n" + "="*70)
    print(" YouTube AI å†…å®¹æ£€æµ‹å·¥å…·")
    print("="*70)

    # æ£€æŸ¥ API key
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° YouTube API å¯†é’¥")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® YOUTUBE_API_KEY")
        return 1

    # åˆå§‹åŒ–æ£€æµ‹å™¨
    try:
        detector = AIContentDetector(api_key)
        print("\nâœ… YouTube API è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1

    # æ”¶é›†è§†é¢‘ ID
    video_ids = []
    if args.video_id:
        video_ids.append(args.video_id)
    elif args.file:
        try:
            with open(args.file, 'r') as f:
                video_ids = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            return 1

    print(f"\nğŸ“Š å‡†å¤‡æ£€æµ‹ {len(video_ids)} ä¸ªè§†é¢‘\n")

    # æ‰§è¡Œæ£€æµ‹
    results = detector.batch_detect(video_ids, verbose=args.verbose)

    # ç»Ÿè®¡ç»“æœ
    ai_count = sum(1 for r in results if r['is_ai_content'])
    non_ai_count = len(results) - ai_count

    print("\n" + "="*70)
    print(" æ£€æµ‹ç»“æœç»Ÿè®¡")
    print("="*70)
    print(f"\næ€»è§†é¢‘æ•°: {len(results)}")
    print(f"AI å†…å®¹: {ai_count} ({ai_count/len(results)*100:.1f}%)")
    print(f"é AI å†…å®¹: {non_ai_count} ({non_ai_count/len(results)*100:.1f}%)")

    # æ˜¾ç¤º AI å†…å®¹åˆ—è¡¨
    if ai_count > 0:
        print(f"\nğŸ¤– æ£€æµ‹åˆ°çš„ AI å†…å®¹è§†é¢‘:")
        for result in results:
            if result['is_ai_content']:
                print(f"\n  è§†é¢‘ ID: {result['video_id']}")
                print(f"  æ ‡é¢˜: {result['title'][:60]}...")
                print(f"  ç½®ä¿¡åº¦: {result['confidence']:.2f}")
                print(f"  æŒ‡æ ‡:")
                ind = result['indicators']
                if ind['official_disclosure']:
                    print(f"    âœ“ å®˜æ–¹ AI å†…å®¹å£°æ˜")
                if ind['ai_tools_mentioned']:
                    print(f"    âœ“ AI å·¥å…·: {', '.join(ind['ai_tools_mentioned'][:3])}")
                if ind['ai_tags']:
                    print(f"    âœ“ AI æ ‡ç­¾: {', '.join(ind['ai_tags'][:3])}")

    # ä¿å­˜ç»“æœ
    if args.output:
        import json
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

    print()
    return 0


if __name__ == '__main__':
    sys.exit(main())
