#!/usr/bin/env python3
"""
ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨ - ç”Ÿæˆå®Œæ•´çš„ç ”ç©¶åˆ†ææŠ¥å‘Š

Usage:
    python scripts/generate_report.py --input data/processed/comments_sentiment_topics.csv
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
from datetime import datetime

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--input', required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default='output/reports', help='è¾“å‡ºç›®å½•')
    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("="*70)
    print(" ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆ")
    print("="*70)
    print(f"\nğŸ“‚ è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")

    # è¯»å–æ•°æ®
    print(f"\nğŸ“Š åŠ è½½æ•°æ®...")
    df = pd.read_csv(input_path)
    print(f"  - è¯„è®ºæ•°é‡: {len(df)}")

    # ç”ŸæˆæŠ¥å‘Š
    report = []

    # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
    report.append("="*70)
    report.append("YouTube Shorts è¯„è®ºåˆ†æ - ç»¼åˆç ”ç©¶æŠ¥å‘Š")
    report.append("="*70)
    report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"æ•°æ®æ–‡ä»¶: {input_path}")
    report.append(f"åˆ†æè¯„è®ºæ•°: {len(df)}")
    report.append("")

    # 1. æ•°æ®æ¦‚è§ˆ
    report.append("="*70)
    report.append("1. æ•°æ®æ¦‚è§ˆ (Data Overview)")
    report.append("="*70)
    report.append("")

    # åŸºæœ¬ç»Ÿè®¡
    report.append("1.1 åŸºæœ¬ç»Ÿè®¡")
    report.append(f"  æ€»è¯„è®ºæ•°: {len(df)}")
    if 'video_id' in df.columns:
        report.append(f"  æ¶‰åŠè§†é¢‘: {df['video_id'].nunique()} ä¸ª")
    if 'author' in df.columns:
        report.append(f"  ç‹¬ç«‹ä½œè€…: {df['author'].nunique()} äºº")
    if 'published_datetime' in df.columns:
        df['published_datetime'] = pd.to_datetime(df['published_datetime'])
        date_range = f"{df['published_datetime'].min().date()} è‡³ {df['published_datetime'].max().date()}"
        report.append(f"  æ—¶é—´è·¨åº¦: {date_range}")
    report.append("")

    # æ–‡æœ¬ç»Ÿè®¡
    if 'text_length' in df.columns:
        report.append("1.2 æ–‡æœ¬ç»Ÿè®¡")
        report.append(f"  å¹³å‡é•¿åº¦: {df['text_length'].mean():.1f} å­—ç¬¦")
        report.append(f"  ä¸­ä½æ•°: {df['text_length'].median():.0f} å­—ç¬¦")
        report.append(f"  æœ€çŸ­: {df['text_length'].min()} å­—ç¬¦")
        report.append(f"  æœ€é•¿: {df['text_length'].max()} å­—ç¬¦")
        if 'word_count' in df.columns:
            report.append(f"  å¹³å‡è¯æ•°: {df['word_count'].mean():.1f}")
        report.append("")

    # äº’åŠ¨ç»Ÿè®¡
    if 'like_count' in df.columns:
        report.append("1.3 äº’åŠ¨ç»Ÿè®¡")
        report.append(f"  æ€»ç‚¹èµæ•°: {df['like_count'].sum()}")
        report.append(f"  å¹³å‡ç‚¹èµ: {df['like_count'].mean():.2f}")
        report.append(f"  ä¸­ä½æ•°: {df['like_count'].median():.0f}")
        report.append(f"  æœ€é«˜ç‚¹èµ: {df['like_count'].max()}")
        if 'reply_count' in df.columns:
            report.append(f"  æ€»å›å¤æ•°: {df['reply_count'].sum()}")
        report.append("")

    # 2. æƒ…æ„Ÿåˆ†æç»“æœ
    if 'sentiment' in df.columns:
        report.append("="*70)
        report.append("2. æƒ…æ„Ÿåˆ†æç»“æœ (Sentiment Analysis)")
        report.append("="*70)
        report.append("")

        sentiment_counts = df['sentiment'].value_counts()
        report.append("2.1 æƒ…æ„Ÿåˆ†å¸ƒ")
        for sentiment, count in sentiment_counts.items():
            percentage = count / len(df) * 100
            report.append(f"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)")
        report.append("")

        # ç½®ä¿¡åº¦ç»Ÿè®¡
        if 'sentiment_confidence' in df.columns:
            report.append("2.2 æƒ…æ„Ÿç½®ä¿¡åº¦")
            report.append(f"  å¹³å‡: {df['sentiment_confidence'].mean():.3f}")
            report.append(f"  ä¸­ä½æ•°: {df['sentiment_confidence'].median():.3f}")
            report.append(f"  èŒƒå›´: {df['sentiment_confidence'].min():.3f} - {df['sentiment_confidence'].max():.3f}")
            report.append("")

        # æƒ…æ„Ÿä¸ç‚¹èµå…³ç³»
        if 'like_count' in df.columns:
            report.append("2.3 æƒ…æ„Ÿä¸äº’åŠ¨å…³ç³»")
            for sentiment in ['positive', 'negative', 'neutral']:
                if sentiment in df['sentiment'].values:
                    subset = df[df['sentiment'] == sentiment]
                    report.append(f"  {sentiment.capitalize()} è¯„è®º:")
                    report.append(f"    å¹³å‡ç‚¹èµ: {subset['like_count'].mean():.2f}")
                    report.append(f"    ä¸­ä½æ•°: {subset['like_count'].median():.0f}")
                    report.append(f"    æœ€é«˜: {subset['like_count'].max()}")
            report.append("")

            # å…³é”®å‘ç°
            pos_avg = df[df['sentiment'] == 'positive']['like_count'].mean() if 'positive' in df['sentiment'].values else 0
            neu_avg = df[df['sentiment'] == 'neutral']['like_count'].mean() if 'neutral' in df['sentiment'].values else 0
            if pos_avg > 0 and neu_avg > 0:
                ratio = pos_avg / neu_avg
                report.append(f"  â­ å…³é”®å‘ç°: ç§¯æè¯„è®ºè·å¾—çš„å¹³å‡ç‚¹èµæ˜¯ä¸­æ€§è¯„è®ºçš„ {ratio:.1f} å€")
                report.append("")

    # 3. ä¸»é¢˜å»ºæ¨¡ç»“æœ
    if 'topic' in df.columns:
        report.append("="*70)
        report.append("3. ä¸»é¢˜å»ºæ¨¡ç»“æœ (Topic Modeling)")
        report.append("="*70)
        report.append("")

        valid_topics = df[df['topic'] >= 0]
        report.append(f"3.1 ä¸»é¢˜åˆ†å¸ƒ (æœ‰æ•ˆä¸»é¢˜åˆ†é…: {len(valid_topics)})")
        topic_counts = valid_topics['topic'].value_counts().sort_index()
        for topic_id, count in topic_counts.items():
            percentage = count / len(df) * 100
            report.append(f"  ä¸»é¢˜ {topic_id}: {count} ({percentage:.1f}%)")
        report.append("")

        # ä¸»é¢˜-æƒ…æ„Ÿäº¤å‰åˆ†æ
        if 'sentiment' in df.columns:
            report.append("3.2 ä¸»é¢˜-æƒ…æ„Ÿäº¤å‰åˆ†æ")
            for topic_id in sorted(valid_topics['topic'].unique()):
                topic_df = df[df['topic'] == topic_id]
                sentiment_dist = topic_df['sentiment'].value_counts()

                report.append(f"\n  ä¸»é¢˜ {topic_id} ({len(topic_df)} æ¡è¯„è®º):")

                # è®¡ç®—ç§¯æç‡
                pos_count = sentiment_dist.get('positive', 0)
                pos_ratio = pos_count / len(topic_df) * 100

                for sentiment, count in sentiment_dist.items():
                    percentage = count / len(topic_df) * 100
                    report.append(f"    {sentiment}: {count} ({percentage:.1f}%)")

                # å¹³å‡ç‚¹èµ
                if 'like_count' in df.columns:
                    avg_likes = topic_df['like_count'].mean()
                    report.append(f"    å¹³å‡ç‚¹èµ: {avg_likes:.2f}")

            report.append("")

    # 4. æ—¶é—´åºåˆ—åˆ†æ
    if 'published_datetime' in df.columns:
        report.append("="*70)
        report.append("4. æ—¶é—´åˆ†æ (Temporal Analysis)")
        report.append("="*70)
        report.append("")

        df['published_date'] = df['published_datetime'].dt.date

        # æ¯æ—¥è¯„è®ºæ•°
        report.append("4.1 æ¯æ—¥è¯„è®ºåˆ†å¸ƒ")
        daily_counts = df['published_date'].value_counts().sort_index()
        for date, count in daily_counts.items():
            report.append(f"  {date}: {count} æ¡")
        report.append("")

        # æ¯æ—¥æƒ…æ„Ÿåˆ†å¸ƒ
        if 'sentiment' in df.columns:
            report.append("4.2 æ¯æ—¥æƒ…æ„Ÿåˆ†å¸ƒ")
            daily_sentiment = df.groupby(['published_date', 'sentiment']).size().unstack(fill_value=0)
            for date in daily_sentiment.index:
                report.append(f"  {date}:")
                for sentiment in daily_sentiment.columns:
                    count = daily_sentiment.loc[date, sentiment]
                    if count > 0:
                        report.append(f"    {sentiment}: {count}")
            report.append("")

    # 5. çƒ­é—¨è¯„è®ºåˆ†æ
    if 'like_count' in df.columns:
        report.append("="*70)
        report.append("5. çƒ­é—¨è¯„è®ºåˆ†æ (Popular Comments)")
        report.append("="*70)
        report.append("")

        text_column = 'text_clean' if 'text_clean' in df.columns else 'text'

        # Top 10 æœ€çƒ­é—¨è¯„è®º
        report.append("5.1 æœ€çƒ­é—¨è¯„è®º (Top 10)")
        top_comments = df.nlargest(10, 'like_count')
        for i, (idx, row) in enumerate(top_comments.iterrows(), 1):
            text = row[text_column][:100] + "..." if len(row[text_column]) > 100 else row[text_column]
            report.append(f"\n  {i}. ğŸ‘ {row['like_count']} èµ")
            if 'sentiment' in df.columns:
                report.append(f"     æƒ…æ„Ÿ: {row['sentiment']}")
            if 'topic' in df.columns and row['topic'] >= 0:
                report.append(f"     ä¸»é¢˜: {int(row['topic'])}")
            report.append(f"     å†…å®¹: {text}")

        report.append("")

    # 6. ç ”ç©¶å‘ç°æ€»ç»“
    report.append("="*70)
    report.append("6. ç ”ç©¶å‘ç°æ€»ç»“ (Key Findings)")
    report.append("="*70)
    report.append("")

    findings = []

    # æƒ…æ„Ÿå‘ç°
    if 'sentiment' in df.columns:
        sentiment_counts = df['sentiment'].value_counts()
        dominant_sentiment = sentiment_counts.idxmax()
        dominant_pct = sentiment_counts.max() / len(df) * 100
        findings.append(f"â€¢ ä¸»å¯¼æƒ…æ„Ÿä¸º {dominant_sentiment} ({dominant_pct:.1f}%)")

        if 'like_count' in df.columns:
            pos_avg = df[df['sentiment'] == 'positive']['like_count'].mean() if 'positive' in df['sentiment'].values else 0
            neu_avg = df[df['sentiment'] == 'neutral']['like_count'].mean() if 'neutral' in df['sentiment'].values else 0
            neg_avg = df[df['sentiment'] == 'negative']['like_count'].mean() if 'negative' in df['sentiment'].values else 0

            if pos_avg > max(neu_avg, neg_avg):
                findings.append(f"â€¢ ç§¯æè¯„è®ºè·å¾—æ›´å¤šäº’åŠ¨ (å¹³å‡ {pos_avg:.2f} èµ)")

    # ä¸»é¢˜å‘ç°
    if 'topic' in df.columns:
        valid_topics = df[df['topic'] >= 0]
        if len(valid_topics) > 0:
            topic_counts = valid_topics['topic'].value_counts()
            largest_topic = topic_counts.idxmax()
            largest_pct = topic_counts.max() / len(df) * 100
            findings.append(f"â€¢ æœ€å¤§ä¸»é¢˜ç¾¤ç»„ä¸ºä¸»é¢˜ {largest_topic} ({largest_pct:.1f}%)")

    # äº’åŠ¨å‘ç°
    if 'like_count' in df.columns:
        high_engagement = (df['like_count'] > df['like_count'].quantile(0.9)).sum()
        high_pct = high_engagement / len(df) * 100
        findings.append(f"â€¢ é«˜äº’åŠ¨è¯„è®º (>90th percentile) å  {high_pct:.1f}%")

    # æ—¶é—´å‘ç°
    if 'published_datetime' in df.columns:
        df['hour'] = df['published_datetime'].dt.hour
        peak_hour = df['hour'].mode()[0] if len(df['hour'].mode()) > 0 else None
        if peak_hour is not None:
            findings.append(f"â€¢ è¯„è®ºé«˜å³°æ—¶æ®µä¸º {peak_hour}:00")

    report.append("\n".join(findings))
    report.append("")

    # 7. æ–¹æ³•è®ºè¯´æ˜
    report.append("="*70)
    report.append("7. æ–¹æ³•è®º (Methodology)")
    report.append("="*70)
    report.append("")
    report.append("7.1 æ•°æ®é‡‡é›†")
    report.append("  - å¹³å°: YouTube Shorts")
    report.append("  - API: YouTube Data API v3")
    report.append("  - é‡‡é›†æ–¹å¼: è‡ªåŠ¨åŒ–çƒ­é—¨è§†é¢‘é‡‡é›†")
    report.append("")
    report.append("7.2 æ•°æ®é¢„å¤„ç†")
    report.append("  - æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–")
    report.append("  - åƒåœ¾è¯„è®ºè¿‡æ»¤")
    report.append("  - é‡å¤è¯„è®ºå»é™¤")
    report.append("  - æ—¶é—´ç‰¹å¾æå–")
    report.append("")
    report.append("7.3 æƒ…æ„Ÿåˆ†æ")
    report.append("  - æ–¹æ³•: åŸºäºè§„åˆ™çš„å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ")
    report.append("  - è¯­è¨€æ”¯æŒ: ä¸­æ–‡ã€è‹±è¯­ã€è¥¿ç­ç‰™è¯­")
    report.append("  - åˆ†ç±»: Positive, Negative, Neutral")
    report.append("")
    report.append("7.4 ä¸»é¢˜å»ºæ¨¡")
    report.append("  - ç®—æ³•: LDA (Latent Dirichlet Allocation)")
    report.append("  - é¢„å¤„ç†: Stop words removal, tokenization")
    report.append("  - ä¸»é¢˜æ•°é‡: 5")
    report.append("")

    # 8. ç»“è®º
    report.append("="*70)
    report.append("8. ç»“è®ºä¸å»ºè®® (Conclusions)")
    report.append("="*70)
    report.append("")
    report.append("æœ¬ç ”ç©¶é€šè¿‡åˆ†æ YouTube Shorts è¯„è®ºï¼Œæ­ç¤ºäº†è§‚ä¼—çš„æƒ…æ„Ÿå€¾å‘ã€è®¨è®º")
    report.append("ä¸»é¢˜å’Œäº’åŠ¨æ¨¡å¼ã€‚ä¸»è¦å‘ç°åŒ…æ‹¬ï¼š")
    report.append("")

    if 'sentiment' in df.columns and 'like_count' in df.columns:
        pos_avg = df[df['sentiment'] == 'positive']['like_count'].mean() if 'positive' in df['sentiment'].values else 0
        neu_avg = df[df['sentiment'] == 'neutral']['like_count'].mean() if 'neutral' in df['sentiment'].values else 0
        if pos_avg > neu_avg:
            report.append("1. ç§¯ææƒ…æ„Ÿè¯„è®ºè·å¾—æ˜¾è‘—æ›´å¤šçš„ç”¨æˆ·äº’åŠ¨ï¼Œè¡¨æ˜æ­£é¢å†…å®¹æ›´å®¹æ˜“å¼•")
            report.append("   èµ·è§‚ä¼—å…±é¸£å’Œæ”¯æŒã€‚")
            report.append("")

    if 'sentiment' in df.columns:
        sentiment_counts = df['sentiment'].value_counts()
        if 'neutral' in sentiment_counts.index and sentiment_counts['neutral'] > sentiment_counts.sum() * 0.5:
            report.append("2. å¤§å¤šæ•°è¯„è®ºå‘ˆç°ä¸­æ€§æƒ…æ„Ÿï¼Œè¯´æ˜è§‚ä¼—ä¸»è¦å‘è¡¨å®¢è§‚è¯„è®ºè€Œéå¼ºçƒˆ")
            report.append("   çš„æƒ…ç»ªååº”ã€‚")
            report.append("")

    if 'topic' in df.columns:
        report.append("3. è¯„è®ºä¸»é¢˜å‘ˆç°å¤šæ ·åŒ–åˆ†å¸ƒï¼Œåæ˜ äº†ä¸åŒè§‚ä¼—ç¾¤ä½“çš„å…´è¶£å’Œå…³æ³¨ç‚¹ã€‚")
        report.append("")

    report.append("å»ºè®®åç»­ç ”ç©¶æ–¹å‘ï¼š")
    report.append("â€¢ æ‰©å¤§æ•°æ®é›†è§„æ¨¡ï¼Œé‡‡é›†æ›´å¤šè§†é¢‘ç±»åˆ«çš„è¯„è®º")
    report.append("â€¢ è¿›è¡Œ AI ç”Ÿæˆå†…å®¹ vs é AI å†…å®¹çš„å¯¹æ¯”åˆ†æ")
    report.append("â€¢ è¿½è¸ªé•¿æœŸæ—¶é—´åºåˆ—æ•°æ® (2022-è‡³ä»Š) çš„æƒ…æ„Ÿæ¼”å˜")
    report.append("â€¢ ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æå‡æƒ…æ„Ÿåˆ†æå’Œä¸»é¢˜å»ºæ¨¡ç²¾åº¦")
    report.append("")

    report.append("="*70)
    report.append("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•")
    report.append("="*70)

    # ä¿å­˜æŠ¥å‘Š
    report_path = output_dir / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))

    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    print("\n" + "="*70)
    print("æŠ¥å‘Šé¢„è§ˆ:")
    print("="*70)
    for line in report[:50]:  # æ˜¾ç¤ºå‰50è¡Œ
        print(line)
    print("\n... (å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹æ–‡ä»¶)")

    print(f"\nğŸ“Š æŠ¥å‘Šç»Ÿè®¡:")
    print(f"  - æ€»è¡Œæ•°: {len(report)}")
    print(f"  - æ–‡ä»¶å¤§å°: {report_path.stat().st_size / 1024:.1f} KB")

    print("\n" + "="*70)
    print("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    print("="*70)
    print()

if __name__ == '__main__':
    main()
