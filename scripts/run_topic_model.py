#!/usr/bin/env python3
"""
ä¸»é¢˜å»ºæ¨¡è„šæœ¬ - æå–è¯„è®ºä¸­çš„æ ¸å¿ƒè®¨è®ºè¯é¢˜

Usage:
    python scripts/run_topic_model.py --input data/processed/comments_sentiment.csv
"""

import sys
import argparse
from pathlib import Path
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src' / 'main' / 'python'))

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from models.topic_model import TopicModel

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œä¸»é¢˜å»ºæ¨¡')
    parser.add_argument('--input', required=True, help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--n-topics', type=int, default=5, help='ä¸»é¢˜æ•°é‡ï¼ˆé»˜è®¤5ï¼‰')
    parser.add_argument('--backend', default='lda', choices=['lda', 'nmf'],
                        help='ä½¿ç”¨çš„ç®—æ³•ï¼ˆldaæˆ–nmfï¼Œé»˜è®¤ldaï¼‰')
    parser.add_argument('--language', default='spanish', choices=['chinese', 'english', 'spanish', 'multilingual'],
                        help='æ–‡æœ¬è¯­è¨€ï¼ˆé»˜è®¤spanishï¼‰')
    args = parser.parse_args()

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = input_path.parent / f"{input_path.stem}_topics{input_path.suffix}"

    print("="*70)
    print(" ä¸»é¢˜å»ºæ¨¡åˆ†æ")
    print("="*70)
    print(f"\nğŸ“‚ è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ğŸ¤– ç®—æ³•: {args.backend.upper()}")
    print(f"ğŸŒ è¯­è¨€: {args.language}")
    print(f"ğŸ“Š ä¸»é¢˜æ•°é‡: {args.n_topics}")

    # è¯»å–æ•°æ®
    print(f"\nğŸ“Š åŠ è½½æ•°æ®...")
    df = pd.read_csv(input_path)
    print(f"  - è¯„è®ºæ•°é‡: {len(df)}")

    # é€‰æ‹©æ–‡æœ¬åˆ—
    text_column = 'text_clean' if 'text_clean' in df.columns else 'text'
    texts = df[text_column].fillna('').tolist()

    # è¿‡æ»¤ç©ºæ–‡æœ¬å’Œè¿‡çŸ­æ–‡æœ¬
    min_length = 10
    valid_indices = [i for i, text in enumerate(texts) if len(text.strip()) >= min_length]
    valid_texts = [texts[i] for i in valid_indices]

    print(f"  - æœ‰æ•ˆæ–‡æœ¬æ•°: {len(valid_texts)} (è¿‡æ»¤äº† {len(texts) - len(valid_texts)} æ¡è¿‡çŸ­æ–‡æœ¬)")

    if len(valid_texts) < args.n_topics:
        print(f"âŒ æœ‰æ•ˆæ–‡æœ¬æ•°é‡ ({len(valid_texts)}) å°‘äºä¸»é¢˜æ•°é‡ ({args.n_topics})")
        print(f"å»ºè®®: å‡å°‘ä¸»é¢˜æ•°é‡æˆ–ä½¿ç”¨æ›´å¤šæ•°æ®")
        sys.exit(1)

    # åˆå§‹åŒ–ä¸»é¢˜æ¨¡å‹
    print(f"\nğŸ”§ åˆå§‹åŒ–ä¸»é¢˜æ¨¡å‹...")
    topic_model = TopicModel(
        n_topics=args.n_topics,
        backend=args.backend,
        language=args.language
    )

    # è®­ç»ƒæ¨¡å‹
    print(f"\nğŸ¯ è®­ç»ƒä¸»é¢˜æ¨¡å‹...")
    print(f"  (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")

    try:
        topic_model.fit(valid_texts)
        print(f"  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    except Exception as e:
        print(f"  âŒ è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)

    # ä¸ºæ‰€æœ‰æ–‡æœ¬åˆ†é…ä¸»é¢˜
    print(f"\nğŸ“‹ åˆ†é…ä¸»é¢˜æ ‡ç­¾...")
    all_topics = []
    all_topic_probs = []

    for i, text in enumerate(texts):
        if i in valid_indices:
            # æœ‰æ•ˆæ–‡æœ¬ï¼Œä½¿ç”¨æ¨¡å‹é¢„æµ‹
            result = topic_model.transform([text])
            all_topics.append(result['topics'][0])
            all_topic_probs.append(result['probabilities'][0])
        else:
            # æ— æ•ˆæ–‡æœ¬ï¼Œåˆ†é… -1
            all_topics.append(-1)
            all_topic_probs.append(0.0)

    # æ·»åŠ ç»“æœåˆ°DataFrame
    df['topic'] = all_topics
    df['topic_probability'] = all_topic_probs
    df['topic_analyzed_at'] = datetime.now().isoformat()

    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8')

    # æ˜¾ç¤ºä¸»é¢˜
    print(f"\n" + "="*70)
    print(" å‘ç°çš„ä¸»é¢˜ (Discovered Topics)")
    print("="*70)

    topics_info = topic_model.get_topics()

    for topic_id, words in topics_info.items():
        if topic_id == -1:
            continue

        # ç»Ÿè®¡è¿™ä¸ªä¸»é¢˜çš„è¯„è®ºæ•°
        topic_count = (df['topic'] == topic_id).sum()
        percentage = topic_count / len(df) * 100

        print(f"\nğŸ“Œ ä¸»é¢˜ {topic_id} ({topic_count} æ¡è¯„è®º, {percentage:.1f}%)")
        print(f"   å…³é”®è¯: {', '.join(words[:10])}")

    # æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨æ€§è¯„è®º
    print(f"\n" + "="*70)
    print(" ä¸»é¢˜ä»£è¡¨æ€§è¯„è®º (Representative Comments)")
    print("="*70)

    for topic_id in range(args.n_topics):
        topic_comments = df[df['topic'] == topic_id].nlargest(3, 'topic_probability')

        if len(topic_comments) == 0:
            continue

        print(f"\nğŸ“Œ ä¸»é¢˜ {topic_id}:")
        for i, (idx, row) in enumerate(topic_comments.iterrows(), 1):
            text = row[text_column][:100] + "..." if len(row[text_column]) > 100 else row[text_column]
            prob = row['topic_probability']
            print(f"  {i}. (æ¦‚ç‡: {prob:.3f}) {text}")

    # æƒ…æ„Ÿä¸ä¸»é¢˜çš„å…³ç³»åˆ†æ
    if 'sentiment' in df.columns:
        print(f"\n" + "="*70)
        print(" æƒ…æ„Ÿ-ä¸»é¢˜å…³ç³»åˆ†æ")
        print("="*70)

        for topic_id in range(args.n_topics):
            topic_df = df[df['topic'] == topic_id]
            if len(topic_df) == 0:
                continue

            sentiment_dist = topic_df['sentiment'].value_counts()
            print(f"\nğŸ“Œ ä¸»é¢˜ {topic_id} æƒ…æ„Ÿåˆ†å¸ƒ:")
            for sentiment, count in sentiment_dist.items():
                percentage = count / len(topic_df) * 100
                print(f"   {sentiment}: {count} ({percentage:.1f}%)")

    # ç»Ÿè®¡æ‘˜è¦
    print(f"\n" + "="*70)
    print(" ç»Ÿè®¡æ‘˜è¦")
    print("="*70)

    print(f"\næ€»è¯„è®ºæ•°: {len(df)}")
    print(f"æœ‰æ•ˆä¸»é¢˜åˆ†é…: {(df['topic'] >= 0).sum()}")
    print(f"æ— æ•ˆæ–‡æœ¬: {(df['topic'] == -1).sum()}")

    topic_dist = df[df['topic'] >= 0]['topic'].value_counts().sort_index()
    print(f"\nä¸»é¢˜åˆ†å¸ƒ:")
    for topic_id, count in topic_dist.items():
        percentage = count / len(df) * 100
        print(f"  ä¸»é¢˜ {topic_id}: {count} ({percentage:.1f}%)")

    print("\n" + "="*70)
    print("âœ… ä¸»é¢˜å»ºæ¨¡å®Œæˆ")
    print("="*70)

    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"  1. å¯è§†åŒ–ä¸»é¢˜: python scripts/visualize_topics.py --input {output_path}")
    print(f"  2. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š: python scripts/generate_report.py --input {output_path}")
    print(f"  3. æ·±å…¥æ¢ç´¢: ä½¿ç”¨ Jupyter notebook åˆ†æä¸»é¢˜-æƒ…æ„Ÿ-äº’åŠ¨å…³ç³»")
    print()

if __name__ == '__main__':
    main()
