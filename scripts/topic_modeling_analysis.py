#!/usr/bin/env python3
"""
ä¸»é¢˜å»ºæ¨¡åˆ†æ - LDA Topic Modeling
å¯¹YouTubeè¯„è®ºè¿›è¡Œä¸»é¢˜åˆ†æ,è¯†åˆ«è®¨è®ºçš„ä¸»è¦è¯é¢˜
"""

import json
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import re

def preprocess_text(text):
    """é¢„å¤„ç†æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    # è½¬å°å†™
    text = text.lower()
    # ç§»é™¤URL
    text = re.sub(r'http\S+|www\S+', '', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦,ä¿ç•™å­—æ¯æ•°å­—å’Œç©ºæ ¼
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    return text

def get_top_words_per_topic(model, feature_names, n_top_words=10):
    """è·å–æ¯ä¸ªä¸»é¢˜çš„topå…³é”®è¯"""
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_indices = topic.argsort()[-n_top_words:][::-1]
        top_words = [feature_names[i] for i in top_indices]
        topics[f"Topic {topic_idx}"] = top_words
    return topics

def generate_topic_labels(topics_words):
    """æ ¹æ®å…³é”®è¯ç”Ÿæˆä¸»é¢˜æ ‡ç­¾"""
    labels = {}

    # é¢„å®šä¹‰çš„ä¸»é¢˜æ¨¡å¼
    patterns = {
        'AI/Technology': ['ai', 'artificial', 'intelligence', 'machine', 'learning',
                         'chatgpt', 'gpt', 'model', 'neural', 'algorithm'],
        'Creative/Art': ['art', 'creative', 'design', 'drawing', 'painting',
                        'artist', 'artwork', 'create', 'style', 'image'],
        'Video Content': ['video', 'content', 'channel', 'watch', 'subscribe',
                         'shorts', 'youtube', 'creator', 'views'],
        'Tutorial/Education': ['tutorial', 'learn', 'how', 'make', 'easy',
                              'step', 'guide', 'tips', 'help', 'show'],
        'Animation': ['animation', 'animated', 'cartoon', 'character',
                     'motion', 'graphics', 'animate'],
        'Music/Sound': ['music', 'song', 'sound', 'audio', 'beat', 'track'],
        'Gaming': ['game', 'play', 'gaming', 'player', 'level'],
        'General Discussion': ['good', 'nice', 'cool', 'awesome', 'great',
                              'amazing', 'best', 'wow', 'like', 'love']
    }

    for topic_name, words in topics_words.items():
        # æ£€æŸ¥æ¯ä¸ªé¢„å®šä¹‰æ¨¡å¼
        best_match = 'Unknown Topic'
        best_score = 0

        for pattern_name, pattern_words in patterns.items():
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            score = sum(1 for w in words if w in pattern_words)
            if score > best_score:
                best_score = score
                best_match = pattern_name

        # å¦‚æœæ²¡æœ‰å¥½çš„åŒ¹é…,ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
        if best_score < 2:
            best_match = f"{words[0]}/{words[1]}/{words[2]}"

        labels[topic_name] = best_match

    return labels

def plot_topic_distribution(lda_output, output_path):
    """ç»˜åˆ¶ä¸»é¢˜åˆ†å¸ƒå›¾"""
    # è®¡ç®—æ¯ä¸ªä¸»é¢˜åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„å¹³å‡æƒé‡
    topic_weights = lda_output.mean(axis=0)

    fig, ax = plt.subplots(figsize=(12, 6))
    topics = [f"Topic {i}" for i in range(len(topic_weights))]

    bars = ax.bar(topics, topic_weights, color='steelblue', alpha=0.7)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=10)

    ax.set_xlabel('Topics', fontsize=12)
    ax.set_ylabel('Average Topic Weight', fontsize=12)
    ax.set_title('Topic Distribution Across All Comments', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ“ ä¿å­˜å›¾è¡¨: {output_path}")

def plot_top_words(topics_words, output_path):
    """ç»˜åˆ¶æ¯ä¸ªä¸»é¢˜çš„topå…³é”®è¯"""
    n_topics = len(topics_words)
    fig, axes = plt.subplots(n_topics, 1, figsize=(12, 3*n_topics))

    if n_topics == 1:
        axes = [axes]

    for idx, (topic_name, words) in enumerate(topics_words.items()):
        ax = axes[idx]
        y_pos = np.arange(len(words))

        ax.barh(y_pos, range(len(words), 0, -1), color='coral', alpha=0.7)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(words)
        ax.invert_yaxis()
        ax.set_xlabel('Importance Rank', fontsize=10)
        ax.set_title(f'{topic_name} - Top Keywords', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ“ ä¿å­˜å›¾è¡¨: {output_path}")

def analyze_topics_by_category(df, lda_output, output_path):
    """åˆ†æAI vs éAIå†…å®¹çš„ä¸»é¢˜åˆ†å¸ƒå·®å¼‚"""
    # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»é¢˜æƒé‡
    df_with_topics = df.copy()
    for i in range(lda_output.shape[1]):
        df_with_topics[f'topic_{i}'] = lda_output[:, i]

    # æŒ‰video_typeåˆ†ç»„ç»Ÿè®¡
    topic_cols = [col for col in df_with_topics.columns if col.startswith('topic_')]

    ai_topics = df_with_topics[df_with_topics['video_type'] == 'ai_generated'][topic_cols].mean()
    non_ai_topics = df_with_topics[df_with_topics['video_type'] == 'non_ai'][topic_cols].mean()

    # ç»˜åˆ¶å¯¹æ¯”å›¾
    fig, ax = plt.subplots(figsize=(12, 6))

    x = np.arange(len(topic_cols))
    width = 0.35

    bars1 = ax.bar(x - width/2, ai_topics, width, label='AI Content', color='lightcoral', alpha=0.8)
    bars2 = ax.bar(x + width/2, non_ai_topics, width, label='Non-AI Content', color='lightblue', alpha=0.8)

    ax.set_xlabel('Topics', fontsize=12)
    ax.set_ylabel('Average Topic Weight', fontsize=12)
    ax.set_title('Topic Distribution: AI vs Non-AI Content', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'Topic {i}' for i in range(len(topic_cols))], rotation=45)
    ax.legend()

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ“ ä¿å­˜å›¾è¡¨: {output_path}")

    return ai_topics, non_ai_topics

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python topic_modeling_analysis.py <è¯„è®ºæ•°æ®æ–‡ä»¶.json> [è¾“å‡ºç›®å½•]")
        sys.exit(1)

    input_file = sys.argv[1]
    output_dir = Path(sys.argv[2]) if len(sys.argv) > 2 else Path('output/topics')
    output_dir.mkdir(parents=True, exist_ok=True)

    print("="*80)
    print(" YouTube Shorts Topic Modeling Analysis")
    print("="*80)
    print(f"åŠ è½½æ•°æ®: {input_file}")

    # åŠ è½½æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        comments = json.load(f)

    df = pd.DataFrame(comments)
    print(f"âœ… åŠ è½½ {len(df)} æ¡è¯„è®º\n")

    # é¢„å¤„ç†æ–‡æœ¬
    print("1ï¸âƒ£ é¢„å¤„ç†è¯„è®ºæ–‡æœ¬...")
    df['text_clean'] = df['text'].apply(preprocess_text)

    # ç§»é™¤ç©ºæ–‡æœ¬
    df_clean = df[df['text_clean'].str.len() > 10].copy()
    print(f"   âœ“ ä¿ç•™ {len(df_clean)} æ¡æœ‰æ•ˆè¯„è®º\n")

    # LDAä¸»é¢˜å»ºæ¨¡
    print("2ï¸âƒ£ æ‰§è¡ŒLDAä¸»é¢˜å»ºæ¨¡...")
    n_topics = 5  # æå–5ä¸ªä¸»é¢˜
    n_top_words = 10

    # ä½¿ç”¨CountVectorizer
    vectorizer = CountVectorizer(
        max_df=0.8,  # å¿½ç•¥å‡ºç°åœ¨80%ä»¥ä¸Šæ–‡æ¡£ä¸­çš„è¯
        min_df=5,     # è‡³å°‘å‡ºç°åœ¨5ä¸ªæ–‡æ¡£ä¸­
        max_features=1000,
        stop_words='english'
    )

    doc_term_matrix = vectorizer.fit_transform(df_clean['text_clean'])
    print(f"   âœ“ æ–‡æ¡£-è¯é¡¹çŸ©é˜µ: {doc_term_matrix.shape}")

    # LDAæ¨¡å‹
    lda_model = LatentDirichletAllocation(
        n_components=n_topics,
        max_iter=50,
        learning_method='online',
        random_state=42,
        n_jobs=-1
    )

    lda_output = lda_model.fit_transform(doc_term_matrix)
    print(f"   âœ“ LDAä¸»é¢˜æ•°: {n_topics}\n")

    # æå–ä¸»é¢˜å…³é”®è¯
    print("3ï¸âƒ£ æå–ä¸»é¢˜å…³é”®è¯...")
    feature_names = vectorizer.get_feature_names_out()
    topics_words = get_top_words_per_topic(lda_model, feature_names, n_top_words)

    # ç”Ÿæˆä¸»é¢˜æ ‡ç­¾
    topic_labels = generate_topic_labels(topics_words)

    for topic_name, words in topics_words.items():
        label = topic_labels[topic_name]
        print(f"   {topic_name} ({label}):")
        print(f"      {', '.join(words[:5])}\n")

    # å¯è§†åŒ–
    print("4ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–...")
    plot_topic_distribution(lda_output, output_dir / 'topic_distribution.png')
    plot_top_words(topics_words, output_dir / 'topic_keywords.png')

    # AI vs éAI ä¸»é¢˜å¯¹æ¯”
    if 'video_type' in df_clean.columns:
        print("\n5ï¸âƒ£ åˆ†æAI vs éAIä¸»é¢˜å·®å¼‚...")
        ai_topics, non_ai_topics = analyze_topics_by_category(
            df_clean, lda_output, output_dir / 'topic_comparison_ai_vs_nonai.png'
        )

    # ç”ŸæˆæŠ¥å‘Š
    print("\n6ï¸âƒ£ ç”Ÿæˆä¸»é¢˜åˆ†ææŠ¥å‘Š...")
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    report = f"""================================================================================
 Topic Modeling Analysis Report
================================================================================

ç”Ÿæˆæ—¶é—´: {timestamp}
åˆ†æè¯„è®ºæ•°: {len(df_clean)}
æå–ä¸»é¢˜æ•°: {n_topics}

================================================================================
ä¸»é¢˜æ¦‚è§ˆ
================================================================================

"""

    for topic_name, words in topics_words.items():
        label = topic_labels[topic_name]
        report += f"{topic_name}: {label}\n"
        report += f"  å…³é”®è¯: {', '.join(words)}\n\n"

    report += """================================================================================
ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡
================================================================================

"""

    topic_weights = lda_output.mean(axis=0)
    for i, weight in enumerate(topic_weights):
        label = topic_labels[f"Topic {i}"]
        report += f"Topic {i} ({label}): {weight:.3f} ({weight*100:.1f}%)\n"

    if 'video_type' in df_clean.columns:
        report += f"""
================================================================================
AI vs éAI ä¸»é¢˜å¯¹æ¯”
================================================================================

AIå†…å®¹ä¸»é¢˜åˆ†å¸ƒ:
"""
        for i, weight in enumerate(ai_topics):
            label = topic_labels[f"Topic {i}"]
            report += f"  Topic {i} ({label}): {weight:.3f}\n"

        report += "\néAIå†…å®¹ä¸»é¢˜åˆ†å¸ƒ:\n"
        for i, weight in enumerate(non_ai_topics):
            label = topic_labels[f"Topic {i}"]
            report += f"  Topic {i} ({label}): {weight:.3f}\n"

    report += f"""
================================================================================
æŠ¥å‘Šç»“æŸ
================================================================================
"""

    # ä¿å­˜æŠ¥å‘Š
    report_file = output_dir / f'topic_analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"   âœ“ ä¿å­˜æŠ¥å‘Š: {report_file}")

    # ä¿å­˜ä¸»é¢˜æ•°æ®ä¸ºJSON
    topic_data = {
        'topics': {
            topic_name: {
                'label': topic_labels[topic_name],
                'keywords': words,
                'weight': float(topic_weights[int(topic_name.split()[1])])
            }
            for topic_name, words in topics_words.items()
        },
        'metadata': {
            'n_topics': n_topics,
            'n_documents': len(df_clean),
            'timestamp': timestamp
        }
    }

    json_file = output_dir / 'topics.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(topic_data, f, indent=2, ensure_ascii=False)
    print(f"   âœ“ ä¿å­˜JSON: {json_file}")

    print("\n" + "="*80)
    print(" âœ… ä¸»é¢˜å»ºæ¨¡åˆ†æå®Œæˆ!")
    print("="*80)
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š å›¾è¡¨: topic_distribution.png, topic_keywords.png")
    if 'video_type' in df_clean.columns:
        print(f"      topic_comparison_ai_vs_nonai.png")
    print(f"ğŸ“„ æŠ¥å‘Š: {report_file.name}")
    print(f"ğŸ“‹ æ•°æ®: topics.json\n")

if __name__ == '__main__':
    main()
