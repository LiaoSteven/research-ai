#!/usr/bin/env python3
"""
è‡ªç„¶åˆ†å¸ƒé‡‡æ ·å™¨ - ä¸é¢„è®¾AI/éAIæ¯”ä¾‹

é‡‡æ ·ç­–ç•¥ï¼š
1. æŒ‰æ—¶é—´å‡åŒ€åˆ†å¸ƒé‡‡æ ·ï¼ˆæ¯å­£åº¦çº¦6,250æ¡è¯„è®ºï¼‰
2. å¯¹æ¯ä¸ªè§†é¢‘è‡ªåŠ¨æ£€æµ‹æ˜¯å¦AIç”Ÿæˆ
3. è‡ªç„¶è·å¾—æ¯ä¸ªå­£åº¦çš„çœŸå®AIå æ¯”
4. AIå æ¯”æœ¬èº«å°±æ˜¯ç ”ç©¶å‘ç°ï¼Œè€Œéé¢„è®¾å‡è®¾

ç ”ç©¶é—®é¢˜ï¼š
- AIè§†é¢‘å æ¯”å¦‚ä½•æ¼”å˜ï¼Ÿï¼ˆæ•°æ®é©±åŠ¨çš„å‘ç°ï¼‰
- è§‚ä¼—å¯¹AIå†…å®¹çš„ååº”å¦‚ä½•å˜åŒ–ï¼Ÿ
- AI vs éAIå†…å®¹çš„å·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ

ä½¿ç”¨æ–¹æ³•ï¼š
    python natural_distribution_collector.py --total 100000 --start-date 2022-01-01 --end-date 2025-10-31
"""

import sys
from pathlib import Path
import argparse
import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import random

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å¯¼å…¥æ¨¡å—
from src.main.python.services.youtube_collector import YouTubeCollector


class AIContentDetector:
    """AIå†…å®¹æ£€æµ‹å™¨ - åŸºäºå…³é”®è¯çš„ç®€å•å®ç°"""

    # AIç›¸å…³å…³é”®è¯ï¼ˆæ ‡é¢˜ã€æè¿°ã€æ ‡ç­¾ï¼‰
    AI_KEYWORDS = [
        # å·¥å…·åç§°
        'ai', 'artificial intelligence', 'chatgpt', 'gpt', 'midjourney',
        'stable diffusion', 'dall-e', 'dalle', 'ai generated', 'ai art',
        'ai animation', 'ai video', 'generative ai', 'sora', 'runway',

        # ä¸­æ–‡å…³é”®è¯
        'aiç”Ÿæˆ', 'aiåˆ¶ä½œ', 'aiåˆ›ä½œ', 'äººå·¥æ™ºèƒ½', 'aiç»˜ç”»', 'aiè§†é¢‘',

        # æè¿°æ€§è¯æ±‡
        'generated by ai', 'created with ai', 'made with ai',
        'ai-generated', 'ai-created', 'machine learning', 'neural network',
        'text to image', 'text to video', 'ai model', 'diffusion model'
    ]

    # æ’é™¤å…³é”®è¯ï¼ˆå‡é˜³æ€§è¿‡æ»¤ï¼‰
    EXCLUDE_KEYWORDS = [
        'against ai', 'anti ai', 'no ai', 'human made', 'real footage',
        'traditional art', 'hand drawn', 'handmade'
    ]

    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.ai_keywords_lower = [kw.lower() for kw in self.AI_KEYWORDS]
        self.exclude_keywords_lower = [kw.lower() for kw in self.EXCLUDE_KEYWORDS]

    def detect(self, video_info: Dict) -> Dict:
        """
        æ£€æµ‹è§†é¢‘æ˜¯å¦AIç”Ÿæˆ

        Args:
            video_info: è§†é¢‘ä¿¡æ¯å­—å…¸

        Returns:
            {
                'is_ai': bool,
                'confidence': float,
                'matched_keywords': list,
                'detection_source': str
            }
        """
        title = video_info.get('title', '').lower()
        description = video_info.get('description', '').lower()
        tags = ' '.join(video_info.get('tags', [])).lower()

        combined_text = f"{title} {description} {tags}"

        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        for exclude_kw in self.exclude_keywords_lower:
            if exclude_kw in combined_text:
                return {
                    'is_ai': False,
                    'confidence': 0.0,
                    'matched_keywords': [],
                    'detection_source': 'excluded',
                    'reason': f'Matched exclude keyword: {exclude_kw}'
                }

        # æ£€æµ‹AIå…³é”®è¯
        matched_keywords = []
        scores = []

        for keyword in self.ai_keywords_lower:
            if keyword in combined_text:
                matched_keywords.append(keyword)

                # æƒé‡è®¡ç®—
                if keyword in title:
                    scores.append(0.5)  # æ ‡é¢˜ä¸­å‡ºç°æƒé‡é«˜
                elif keyword in description[:200]:  # æè¿°å‰200å­—ç¬¦
                    scores.append(0.3)
                elif keyword in tags:
                    scores.append(0.4)
                else:
                    scores.append(0.2)

        # è®¡ç®—ç½®ä¿¡åº¦
        if not matched_keywords:
            confidence = 0.0
            is_ai = False
        else:
            # ç½®ä¿¡åº¦ = å…³é”®è¯æ•°é‡ Ã— å¹³å‡æƒé‡
            confidence = min(len(matched_keywords) * sum(scores) / len(scores), 1.0)
            is_ai = confidence >= 0.2  # é˜ˆå€¼ï¼š0.2

        return {
            'is_ai': is_ai,
            'confidence': round(confidence, 3),
            'matched_keywords': matched_keywords[:5],  # æœ€å¤šè®°å½•5ä¸ª
            'detection_source': 'keyword_matching',
            'num_matches': len(matched_keywords)
        }


class NaturalDistributionCollector:
    """è‡ªç„¶åˆ†å¸ƒé‡‡æ ·å™¨ - ä¸é¢„è®¾AIæ¯”ä¾‹"""

    # é€šç”¨æœç´¢å…³é”®è¯ï¼ˆä¸åå‘AIæˆ–éAIï¼‰
    GENERAL_QUERIES = [
        # çƒ­é—¨çŸ­è§†é¢‘ç±»åˆ«
        'shorts', 'viral shorts', 'trending shorts',
        'popular shorts', 'funny shorts', 'creative shorts',

        # å†…å®¹ç±»å‹
        'art shorts', 'animation shorts', 'video shorts',
        'tutorial shorts', 'educational shorts', 'entertainment shorts',

        # å¹´ä»½å…³é”®è¯ï¼ˆç”¨äºæ—¶é—´è¿‡æ»¤ï¼‰
        'shorts 2022', 'shorts 2023', 'shorts 2024', 'shorts 2025',

        # åˆ›ä½œç›¸å…³
        'creative content', 'content creation', 'video creation',
        'digital art', 'visual effects', 'motion graphics'
    ]

    def __init__(self, api_key: str):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.collector = YouTubeCollector(api_key=api_key)
        self.detector = AIContentDetector()
        self.youtube = self.collector.youtube

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_videos_searched': 0,
            'total_comments_collected': 0,
            'ai_videos': 0,
            'non_ai_videos': 0,
            'detection_failures': 0,
            'quarters_completed': 0
        }

    def search_videos_by_quarter(
        self,
        start_date: datetime,
        end_date: datetime,
        max_videos: int = 200
    ) -> List[str]:
        """
        æŒ‰å­£åº¦æœç´¢è§†é¢‘ï¼ˆä¸åå‘ä»»ä½•ç±»å‹ï¼‰

        Args:
            start_date: å­£åº¦èµ·å§‹æ—¥æœŸ
            end_date: å­£åº¦ç»“æŸæ—¥æœŸ
            max_videos: æœ€å¤šæœç´¢è§†é¢‘æ•°

        Returns:
            è§†é¢‘IDåˆ—è¡¨
        """
        video_ids = []
        found_videos = set()

        # RFC 3339 æ ¼å¼
        published_after = start_date.strftime('%Y-%m-%dT00:00:00Z')
        published_before = end_date.strftime('%Y-%m-%dT23:59:59Z')

        print(f"\nğŸ” æœç´¢è§†é¢‘ [{start_date.date()} ~ {end_date.date()}]")

        # ä½¿ç”¨å¤šç§æœç´¢ç­–ç•¥
        search_strategies = [
            {'order': 'relevance', 'queries': self.GENERAL_QUERIES[:5]},
            {'order': 'viewCount', 'queries': self.GENERAL_QUERIES[5:10]},
            {'order': 'date', 'queries': self.GENERAL_QUERIES[10:]}
        ]

        for strategy in search_strategies:
            if len(video_ids) >= max_videos:
                break

            for query in strategy['queries']:
                if len(video_ids) >= max_videos:
                    break

                try:
                    search_params = {
                        'part': 'id,snippet',
                        'type': 'video',
                        'q': query,
                        'videoDuration': 'short',
                        'publishedAfter': published_after,
                        'publishedBefore': published_before,
                        'maxResults': min(50, max_videos - len(video_ids)),
                        'order': strategy['order'],
                        'regionCode': 'US'
                    }

                    request = self.youtube.search().list(**search_params)
                    response = request.execute()

                    for item in response.get('items', []):
                        if 'videoId' in item['id']:
                            video_id = item['id']['videoId']
                            if video_id not in found_videos:
                                found_videos.add(video_id)
                                video_ids.append(video_id)

                    time.sleep(1)  # APIé™æµ

                except Exception as e:
                    print(f"   âš  æœç´¢é”™è¯¯: {e}")
                    continue

        print(f"   âœ“ æ‰¾åˆ° {len(video_ids)} ä¸ªè§†é¢‘")
        self.stats['total_videos_searched'] += len(video_ids)
        return video_ids

    def collect_quarter(
        self,
        quarter_key: str,
        start_date: datetime,
        end_date: datetime,
        target_comments: int,
        comments_per_video: int = 30
    ) -> Tuple[List, Dict]:
        """
        é‡‡é›†å•ä¸ªå­£åº¦çš„æ•°æ®

        Args:
            quarter_key: å­£åº¦æ ‡è¯†ï¼ˆå¦‚ 2022Q1ï¼‰
            start_date: èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            target_comments: ç›®æ ‡è¯„è®ºæ•°
            comments_per_video: æ¯è§†é¢‘è¯„è®ºæ•°

        Returns:
            (comments, quarter_stats)
        """
        print("\n" + "="*80)
        print(f" é‡‡é›†å­£åº¦: {quarter_key}")
        print("="*80)
        print(f" æ—¥æœŸ: {start_date.date()} ~ {end_date.date()}")
        print(f" ç›®æ ‡: {target_comments:,} æ¡è¯„è®º")
        print("="*80)

        # ä¼°ç®—éœ€è¦çš„è§†é¢‘æ•°
        videos_needed = (target_comments // comments_per_video) * 2  # å¤šæœç´¢å¤‡ç”¨

        # æœç´¢è§†é¢‘
        video_ids = self.search_videos_by_quarter(
            start_date, end_date, videos_needed
        )

        if not video_ids:
            print("âŒ æœªæ‰¾åˆ°è§†é¢‘")
            return [], {}

        # éšæœºæ‰“ä¹±ï¼ˆé¿å…åå·®ï¼‰
        random.shuffle(video_ids)

        # é‡‡é›†è¯„è®º
        all_comments = []
        quarter_stats = {
            'quarter': quarter_key,
            'start_date': start_date.isoformat(),
            'end_date': end_date.isoformat(),
            'target_comments': target_comments,
            'collected_comments': 0,
            'videos_processed': 0,
            'ai_videos': 0,
            'non_ai_videos': 0,
            'ai_comments': 0,
            'non_ai_comments': 0,
            'detection_details': []
        }

        print(f"\nğŸ“ å¼€å§‹é‡‡é›†è¯„è®º...")

        for idx, video_id in enumerate(video_ids, 1):
            if len(all_comments) >= target_comments:
                print(f"\nâœ… å·²è¾¾åˆ°ç›®æ ‡è¯„è®ºæ•° {target_comments:,}")
                break

            try:
                # è·å–è§†é¢‘ä¿¡æ¯
                video_info = self.collector.get_video_info(video_id)

                # AIæ£€æµ‹
                detection_result = self.detector.detect(video_info)

                video_type = 'ai_generated' if detection_result['is_ai'] else 'non_ai'

                # æ›´æ–°ç»Ÿè®¡
                if detection_result['is_ai']:
                    quarter_stats['ai_videos'] += 1
                    self.stats['ai_videos'] += 1
                else:
                    quarter_stats['non_ai_videos'] += 1
                    self.stats['non_ai_videos'] += 1

                # è·å–è¯„è®º
                comments = self.collector.get_video_comments(
                    video_id,
                    max_comments=min(comments_per_video, target_comments - len(all_comments)),
                    include_replies=True
                )

                # ä¸ºè¯„è®ºæ·»åŠ æ ‡ç­¾
                for comment in comments:
                    comment['quarter'] = quarter_key
                    comment['video_type'] = video_type
                    comment['ai_detection'] = detection_result

                all_comments.extend(comments)

                # æ›´æ–°ç»Ÿè®¡
                if detection_result['is_ai']:
                    quarter_stats['ai_comments'] += len(comments)
                else:
                    quarter_stats['non_ai_comments'] += len(comments)

                quarter_stats['videos_processed'] += 1

                # è®°å½•æ£€æµ‹è¯¦æƒ…
                quarter_stats['detection_details'].append({
                    'video_id': video_id,
                    'title': video_info['title'][:50],
                    'video_type': video_type,
                    'confidence': detection_result['confidence'],
                    'comments_collected': len(comments)
                })

                # è¿›åº¦æ˜¾ç¤º
                ai_ratio = quarter_stats['ai_comments'] / len(all_comments) * 100 if all_comments else 0
                print(f"  [{idx}/{len(video_ids)}] {video_id} | {video_type} "
                      f"(ç½®ä¿¡åº¦:{detection_result['confidence']:.2f}) | "
                      f"{len(comments)}æ¡è¯„è®º | æ€»è®¡:{len(all_comments):,} | AIå æ¯”:{ai_ratio:.1f}%")

                time.sleep(0.5)

            except Exception as e:
                print(f"  âœ— è§†é¢‘ {video_id} å¤±è´¥: {e}")
                self.stats['detection_failures'] += 1
                continue

        quarter_stats['collected_comments'] = len(all_comments)
        self.stats['total_comments_collected'] += len(all_comments)
        self.stats['quarters_completed'] += 1

        # è®¡ç®—å­£åº¦AIå æ¯”
        ai_ratio = (quarter_stats['ai_comments'] / quarter_stats['collected_comments'] * 100
                   if quarter_stats['collected_comments'] > 0 else 0)

        print(f"\nâœ… {quarter_key} å®Œæˆ:")
        print(f"   è¯„è®ºæ€»æ•°: {quarter_stats['collected_comments']:,}")
        print(f"   AIå†…å®¹: {quarter_stats['ai_comments']:,} ({ai_ratio:.1f}%)")
        print(f"   éAI: {quarter_stats['non_ai_comments']:,} ({100-ai_ratio:.1f}%)")
        print(f"   å¤„ç†è§†é¢‘: {quarter_stats['videos_processed']}")

        return all_comments, quarter_stats

    def collect_all(
        self,
        start_date: str,
        end_date: str,
        total_comments: int,
        output_dir: Path
    ) -> Dict:
        """
        é‡‡é›†æ‰€æœ‰æ•°æ®

        Args:
            start_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            total_comments: ç›®æ ‡æ€»è¯„è®ºæ•°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            é‡‡é›†ç»“æœç»Ÿè®¡
        """
        output_dir.mkdir(parents=True, exist_ok=True)

        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')

        # ç”Ÿæˆå­£åº¦åˆ—è¡¨
        quarters = self._generate_quarters(start_dt, end_dt)
        comments_per_quarter = total_comments // len(quarters)

        print("\n" + "="*80)
        print(" è‡ªç„¶åˆ†å¸ƒé‡‡æ · - æ•°æ®é‡‡é›†å¼€å§‹")
        print("="*80)
        print(f" æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        print(f" ç›®æ ‡è¯„è®º: {total_comments:,} æ¡")
        print(f" å­£åº¦æ•°é‡: {len(quarters)}")
        print(f" æ¯å­£åº¦: ~{comments_per_quarter:,} æ¡")
        print(" ç­–ç•¥: ä¸é¢„è®¾AIæ¯”ä¾‹ï¼Œè®©æ•°æ®è‡ªç„¶åˆ†å¸ƒ")
        print("="*80)

        all_comments = []
        all_quarter_stats = []

        for idx, quarter_info in enumerate(quarters, 1):
            print(f"\nè¿›åº¦: [{idx}/{len(quarters)}] å­£åº¦")

            try:
                comments, quarter_stats = self.collect_quarter(
                    quarter_info['key'],
                    quarter_info['start'],
                    quarter_info['end'],
                    comments_per_quarter,
                    comments_per_video=30
                )

                all_comments.extend(comments)
                all_quarter_stats.append(quarter_stats)

                # ä¿å­˜å­£åº¦æ£€æŸ¥ç‚¹
                self._save_checkpoint(output_dir, comments, quarter_stats)

                # æ˜¾ç¤ºç´¯è®¡è¿›åº¦
                self._print_cumulative_progress(all_quarter_stats, total_comments)

            except Exception as e:
                print(f"\nâŒ å­£åº¦ {quarter_info['key']} å¤±è´¥: {e}")
                continue

        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_result = self._save_final_results(
            output_dir, all_comments, all_quarter_stats
        )

        return final_result

    def _generate_quarters(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """ç”Ÿæˆå­£åº¦åˆ—è¡¨"""
        quarters = []
        current = start_date

        while current <= end_date:
            year = current.year
            month = current.month
            quarter = (month - 1) // 3 + 1

            q_start = datetime(year, (quarter-1)*3 + 1, 1)
            if quarter < 4:
                q_end = datetime(year, quarter*3 + 1, 1) - timedelta(days=1)
            else:
                q_end = datetime(year, 12, 31)

            q_start = max(q_start, start_date)
            q_end = min(q_end, end_date)

            quarters.append({
                'key': f"{year}Q{quarter}",
                'start': q_start,
                'end': q_end
            })

            if quarter < 4:
                current = datetime(year, quarter*3 + 1, 1)
            else:
                current = datetime(year + 1, 1, 1)

        return quarters

    def _save_checkpoint(self, output_dir: Path, comments: List, quarter_stats: Dict):
        """ä¿å­˜å­£åº¦æ£€æŸ¥ç‚¹"""
        checkpoint_file = output_dir / f"checkpoint_{quarter_stats['quarter']}.json"
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump({
                'comments': comments,
                'stats': quarter_stats,
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)

    def _print_cumulative_progress(self, all_stats: List[Dict], total_target: int):
        """æ‰“å°ç´¯è®¡è¿›åº¦"""
        total_collected = sum(s['collected_comments'] for s in all_stats)
        total_ai = sum(s['ai_comments'] for s in all_stats)
        total_non_ai = sum(s['non_ai_comments'] for s in all_stats)
        overall_ai_ratio = (total_ai / total_collected * 100) if total_collected > 0 else 0

        print(f"\nğŸ“Š ç´¯è®¡è¿›åº¦:")
        print(f"   å·²é‡‡é›†: {total_collected:,} / {total_target:,} ({total_collected/total_target*100:.1f}%)")
        print(f"   AIå†…å®¹: {total_ai:,} ({overall_ai_ratio:.1f}%)")
        print(f"   éAI: {total_non_ai:,} ({100-overall_ai_ratio:.1f}%)")
        print(f"   å®Œæˆå­£åº¦: {len(all_stats)}")

    def _save_final_results(
        self, output_dir: Path, comments: List, quarter_stats: List[Dict]
    ) -> Dict:
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜æ‰€æœ‰è¯„è®º
        comments_file = output_dir / f'comments_natural_distribution_{timestamp}.json'
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(comments, f, ensure_ascii=False, indent=2)

        # åˆ†ç¦»AIå’ŒéAI
        ai_comments = [c for c in comments if c.get('video_type') == 'ai_generated']
        non_ai_comments = [c for c in comments if c.get('video_type') == 'non_ai']

        ai_file = output_dir / f'comments_ai_{timestamp}.json'
        non_ai_file = output_dir / f'comments_non_ai_{timestamp}.json'

        with open(ai_file, 'w', encoding='utf-8') as f:
            json.dump(ai_comments, f, ensure_ascii=False, indent=2)
        with open(non_ai_file, 'w', encoding='utf-8') as f:
            json.dump(non_ai_comments, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'collection_timestamp': datetime.now().isoformat(),
            'total_comments': len(comments),
            'ai_comments': len(ai_comments),
            'non_ai_comments': len(non_ai_comments),
            'overall_ai_ratio': len(ai_comments) / len(comments) if comments else 0,
            'quarter_stats': quarter_stats,
            'collection_stats': self.stats,
            'files': {
                'all_comments': str(comments_file),
                'ai_comments': str(ai_file),
                'non_ai_comments': str(non_ai_file)
            }
        }

        metadata_file = output_dir / f'metadata_natural_distribution_{timestamp}.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        self._print_final_report(metadata)

        return metadata

    def _print_final_report(self, metadata: Dict):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        print("\n" + "="*80)
        print(" ğŸ‰ æ•°æ®é‡‡é›†å®Œæˆï¼")
        print("="*80)

        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»è¯„è®ºæ•°: {metadata['total_comments']:,}")
        print(f"   AIå†…å®¹: {metadata['ai_comments']:,} ({metadata['overall_ai_ratio']*100:.1f}%)")
        print(f"   éAI: {metadata['non_ai_comments']:,} ({(1-metadata['overall_ai_ratio'])*100:.1f}%)")

        print(f"\nğŸ“ˆ AIå æ¯”æ¼”å˜ (æŒ‰å­£åº¦):")
        print(f"   {'å­£åº¦':<10} {'AIè¯„è®º':<10} {'æ€»è¯„è®º':<10} {'AIå æ¯”':<10}")
        print(f"   {'-'*40}")
        for qstat in metadata['quarter_stats']:
            ai_ratio = (qstat['ai_comments'] / qstat['collected_comments'] * 100
                       if qstat['collected_comments'] > 0 else 0)
            print(f"   {qstat['quarter']:<10} {qstat['ai_comments']:<10} "
                  f"{qstat['collected_comments']:<10} {ai_ratio:<10.1f}%")

        print(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜:")
        print(f"   å…¨éƒ¨è¯„è®º: {metadata['files']['all_comments']}")
        print(f"   AIè¯„è®º: {metadata['files']['ai_comments']}")
        print(f"   éAIè¯„è®º: {metadata['files']['non_ai_comments']}")

        print(f"\nğŸ”¬ å…³é”®å‘ç°:")
        print(f"   â€¢ æ€»ä½“AIå æ¯”: {metadata['overall_ai_ratio']*100:.1f}%")
        print(f"   â€¢ è¿™æ˜¯çœŸå®ä¸–ç•Œåˆ†å¸ƒï¼Œè€Œéé¢„è®¾å‡è®¾")
        print(f"   â€¢ AIå æ¯”çš„æ—¶é—´æ¼”å˜è¶‹åŠ¿å¯åœ¨åˆ†æä¸­å‘ç°")

        print(f"\nâœ¨ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ•°æ®é¢„å¤„ç†: python scripts/preprocess_data.py")
        print(f"   2. AIå æ¯”è¶‹åŠ¿åˆ†æ: python scripts/analyze_ai_ratio_evolution.py")
        print(f"   3. æƒ…æ„Ÿåˆ†æ: python scripts/run_sentiment_analysis.py")
        print(f"   4. ä¸»é¢˜å»ºæ¨¡: python scripts/run_topic_modeling.py")
        print()


def main():
    parser = argparse.ArgumentParser(
        description='è‡ªç„¶åˆ†å¸ƒé‡‡æ · - ä¸é¢„è®¾AI/éAIæ¯”ä¾‹'
    )
    parser.add_argument('--total', type=int, default=100000,
                       help='ç›®æ ‡æ€»è¯„è®ºæ•° (é»˜è®¤ 100,000)')
    parser.add_argument('--start-date', type=str, default='2022-01-01',
                       help='èµ·å§‹æ—¥æœŸ YYYY-MM-DD')
    parser.add_argument('--end-date', type=str, default='2025-10-31',
                       help='ç»“æŸæ—¥æœŸ YYYY-MM-DD')
    parser.add_argument('--output-dir', type=str, default='data/raw',
                       help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # æ£€æŸ¥ API key
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("\nâŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° YouTube API å¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export YOUTUBE_API_KEY=your_key_here")
        return 1

    # åˆå§‹åŒ–é‡‡é›†å™¨
    try:
        collector = NaturalDistributionCollector(api_key=api_key)
        print("\nâœ… YouTube API è¿æ¥æˆåŠŸ")
        print("âœ… AIæ£€æµ‹å™¨å·²åŠ è½½")
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1

    # å¼€å§‹é‡‡é›†
    try:
        result = collector.collect_all(
            start_date=args.start_date,
            end_date=args.end_date,
            total_comments=args.total,
            output_dir=Path(args.output_dir)
        )
        return 0
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ é‡‡é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
