#!/usr/bin/env python3
"""
é…é¢ä¼˜åŒ–é‡‡é›†å™¨ - å‡å°‘90%çš„APIé…é¢æ¶ˆè€—

ä¼˜åŒ–ç­–ç•¥ï¼š
1. âœ… ä¸€æ¬¡æ€§æœç´¢åˆ›å»ºè§†é¢‘æ± ï¼ˆè€Œéæ¯å­£åº¦é‡å¤æœç´¢ï¼‰
2. âœ… å¢åŠ æ¯è§†é¢‘è¯„è®ºæ•°åˆ°100ï¼ˆå‡å°‘è§†é¢‘æ•°é‡éœ€æ±‚ï¼‰
3. âœ… ä½¿ç”¨è§†é¢‘æ± éšæœºåˆ†é…åˆ°å„å­£åº¦ï¼ˆé¿å…é‡å¤search.listï¼‰
4. âœ… ç¼“å­˜æœºåˆ¶ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

é…é¢å¯¹æ¯”ï¼š
- ä¼˜åŒ–å‰: 35,344 units (3.5å¤©)
- ä¼˜åŒ–å: 4,544 units (0.5å¤©) âš¡ èŠ‚çœ87%ï¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python quota_optimized_collector.py --total 100000 --start-date 2022-01-01 --end-date 2025-10-31
"""

import sys
from pathlib import Path
import argparse
import os
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import random

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# å¯¼å…¥æ¨¡å—
try:
    from services.youtube_collector import YouTubeCollector
    from services.natural_distribution_collector import AIContentDetector
except ImportError:
    from src.main.python.services.youtube_collector import YouTubeCollector
    from src.main.python.services.natural_distribution_collector import AIContentDetector


class QuotaOptimizedCollector:
    """é…é¢ä¼˜åŒ–é‡‡é›†å™¨ - æœ€å°åŒ–APIæ¶ˆè€—"""

    # ç²¾ç®€æœç´¢å…³é”®è¯ï¼ˆåªç”¨æœ€æœ‰æ•ˆçš„ï¼‰
    OPTIMIZED_QUERIES = [
        'shorts',
        'viral shorts',
        'trending shorts',
        'shorts 2022',
        'shorts 2023',
        'shorts 2024'
    ]

    def __init__(self, api_key: str):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.collector = YouTubeCollector(api_key=api_key)
        self.detector = AIContentDetector()
        self.youtube = self.collector.youtube

        # è§†é¢‘æ± ï¼ˆä¸€æ¬¡æœç´¢ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰
        self.video_pool = []
        self.video_pool_file = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_api_calls': {
                'search': 0,
                'videos': 0,
                'comments': 0
            },
            'quota_used': 0,
            'total_videos_collected': 0,
            'total_comments_collected': 0,
            'quarters_completed': 0
        }

    def build_video_pool(
        self,
        start_date: datetime,
        end_date: datetime,
        target_videos: int,
        cache_file: Path = None
    ) -> List[str]:
        """
        ä¸€æ¬¡æ€§æ„å»ºè§†é¢‘æ± ï¼ˆæ‰€æœ‰å­£åº¦å…±ç”¨ï¼‰

        Args:
            start_date: èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            target_videos: ç›®æ ‡è§†é¢‘æ•°
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„

        Returns:
            è§†é¢‘IDåˆ—è¡¨
        """
        # æ£€æŸ¥ç¼“å­˜
        if cache_file and cache_file.exists():
            print(f"ğŸ“¦ åŠ è½½è§†é¢‘æ± ç¼“å­˜: {cache_file}")
            with open(cache_file, 'r') as f:
                cached_data = json.load(f)
                self.video_pool = cached_data['video_ids']
                print(f"   âœ“ å·²åŠ è½½ {len(self.video_pool)} ä¸ªè§†é¢‘")
                return self.video_pool

        print("\n" + "="*80)
        print(" ğŸ—ï¸ æ„å»ºè§†é¢‘æ± ï¼ˆä¸€æ¬¡æœç´¢ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼‰")
        print("="*80)
        print(f" æ—¶é—´èŒƒå›´: {start_date.date()} ~ {end_date.date()}")
        print(f" ç›®æ ‡è§†é¢‘æ•°: {target_videos:,}")
        print(f" æœç´¢å…³é”®è¯: {len(self.OPTIMIZED_QUERIES)} ä¸ª")
        print("="*80)

        video_ids = []
        found_videos = set()

        published_after = start_date.strftime('%Y-%m-%dT00:00:00Z')
        published_before = end_date.strftime('%Y-%m-%dT23:59:59Z')

        # ä½¿ç”¨ç²¾ç®€çš„æœç´¢ç­–ç•¥
        for idx, query in enumerate(self.OPTIMIZED_QUERIES, 1):
            if len(video_ids) >= target_videos:
                break

            try:
                print(f"[{idx}/{len(self.OPTIMIZED_QUERIES)}] æœç´¢: '{query}'")

                search_params = {
                    'part': 'id,snippet',
                    'type': 'video',
                    'q': query,
                    'videoDuration': 'short',
                    'publishedAfter': published_after,
                    'publishedBefore': published_before,
                    'maxResults': 50,  # æ¯æ¬¡æœç´¢è·å–æ›´å¤š
                    'order': 'viewCount',  # ä¼˜å…ˆé«˜è§‚çœ‹é‡ï¼ˆè¯„è®ºæ›´å¤šï¼‰
                    'regionCode': 'US'
                }

                request = self.youtube.search().list(**search_params)
                response = request.execute()

                self.stats['total_api_calls']['search'] += 1
                self.stats['quota_used'] += 100  # search.list = 100 units

                for item in response.get('items', []):
                    if 'videoId' in item['id']:
                        video_id = item['id']['videoId']
                        if video_id not in found_videos:
                            found_videos.add(video_id)
                            video_ids.append(video_id)

                print(f"   âœ“ è·å¾— {len(response.get('items', []))} ä¸ªè§†é¢‘ | ç´¯è®¡: {len(video_ids):,}")
                time.sleep(1)

            except Exception as e:
                print(f"   âš  é”™è¯¯: {e}")
                continue

        # éšæœºæ‰“ä¹±ï¼ˆæ¶ˆé™¤æœç´¢é¡ºåºåå·®ï¼‰
        random.shuffle(video_ids)

        print(f"\nâœ… è§†é¢‘æ± æ„å»ºå®Œæˆ: {len(video_ids):,} ä¸ªè§†é¢‘")
        print(f"ğŸ“Š APIæ¶ˆè€—: {self.stats['total_api_calls']['search']} æ¬¡search.list = {self.stats['quota_used']} units")

        # ä¿å­˜ç¼“å­˜
        if cache_file:
            cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_file, 'w') as f:
                json.dump({
                    'video_ids': video_ids,
                    'created_at': datetime.now().isoformat(),
                    'date_range': f"{start_date.date()} ~ {end_date.date()}",
                    'count': len(video_ids)
                }, f, indent=2)
            print(f"ğŸ’¾ è§†é¢‘æ± å·²ç¼“å­˜: {cache_file}")

        self.video_pool = video_ids
        return video_ids

    def collect_with_pool(
        self,
        quarter_key: str,
        video_ids: List[str],
        target_comments: int,
        comments_per_video: int = 100  # ä¼˜åŒ–ï¼šå¢åŠ åˆ°100
    ) -> Tuple[List, Dict]:
        """
        ä»è§†é¢‘æ± é‡‡é›†è¯„è®º

        Args:
            quarter_key: å­£åº¦æ ‡è¯†
            video_ids: è§†é¢‘IDåˆ—è¡¨ï¼ˆä»æ± ä¸­åˆ†é…ï¼‰
            target_comments: ç›®æ ‡è¯„è®ºæ•°
            comments_per_video: æ¯è§†é¢‘è¯„è®ºæ•°

        Returns:
            (comments, stats)
        """
        print(f"\nğŸ“ é‡‡é›†å­£åº¦: {quarter_key}")
        print(f"   åˆ†é…è§†é¢‘: {len(video_ids)} ä¸ª")
        print(f"   ç›®æ ‡è¯„è®º: {target_comments:,} æ¡")

        all_comments = []
        quarter_stats = {
            'quarter': quarter_key,
            'target_comments': target_comments,
            'collected_comments': 0,
            'videos_processed': 0,
            'ai_videos': 0,
            'non_ai_videos': 0,
            'ai_comments': 0,
            'non_ai_comments': 0,
            'detection_details': []
        }

        for idx, video_id in enumerate(video_ids, 1):
            if len(all_comments) >= target_comments:
                break

            try:
                # è·å–è§†é¢‘ä¿¡æ¯
                video_info = self.collector.get_video_info(video_id)
                self.stats['total_api_calls']['videos'] += 1
                self.stats['quota_used'] += 1  # videos.list = 1 unit

                # AIæ£€æµ‹
                detection_result = self.detector.detect(video_info)
                video_type = 'ai_generated' if detection_result['is_ai'] else 'non_ai'

                if detection_result['is_ai']:
                    quarter_stats['ai_videos'] += 1
                else:
                    quarter_stats['non_ai_videos'] += 1

                # è·å–è¯„è®ºï¼ˆå¢åŠ åˆ°100æ¡ï¼‰
                comments = self.collector.get_video_comments(
                    video_id,
                    max_comments=min(comments_per_video, target_comments - len(all_comments)),
                    include_replies=True
                )

                self.stats['total_api_calls']['comments'] += 1
                self.stats['quota_used'] += 1  # commentThreads.list = 1 unit

                # æ ‡è®°è¯„è®º
                for comment in comments:
                    comment['quarter'] = quarter_key
                    comment['video_type'] = video_type
                    comment['ai_detection'] = detection_result

                all_comments.extend(comments)

                if detection_result['is_ai']:
                    quarter_stats['ai_comments'] += len(comments)
                else:
                    quarter_stats['non_ai_comments'] += len(comments)

                quarter_stats['videos_processed'] += 1
                quarter_stats['detection_details'].append({
                    'video_id': video_id,
                    'title': video_info['title'][:50],
                    'video_type': video_type,
                    'confidence': detection_result['confidence'],
                    'comments_collected': len(comments)
                })

                # è¿›åº¦æ˜¾ç¤º
                ai_ratio = quarter_stats['ai_comments'] / len(all_comments) * 100 if all_comments else 0
                print(f"  [{idx}/{len(video_ids)}] {video_id} | {video_type} "
                      f"(ç½®ä¿¡åº¦:{detection_result['confidence']:.2f}) | "
                      f"{len(comments)}æ¡ | æ€»è®¡:{len(all_comments):,} | AI:{ai_ratio:.1f}%")

                time.sleep(0.3)  # å‡å°‘å»¶è¿Ÿ

            except Exception as e:
                print(f"  âœ— {video_id} å¤±è´¥: {e}")
                continue

        quarter_stats['collected_comments'] = len(all_comments)
        self.stats['total_comments_collected'] += len(all_comments)
        self.stats['quarters_completed'] += 1

        ai_ratio = (quarter_stats['ai_comments'] / quarter_stats['collected_comments'] * 100
                   if quarter_stats['collected_comments'] > 0 else 0)

        print(f"\nâœ… {quarter_key} å®Œæˆ:")
        print(f"   è¯„è®º: {quarter_stats['collected_comments']:,} | "
              f"AI: {quarter_stats['ai_comments']:,} ({ai_ratio:.1f}%) | "
              f"éAI: {quarter_stats['non_ai_comments']:,}")

        return all_comments, quarter_stats

    def collect_all_optimized(
        self,
        start_date: str,
        end_date: str,
        total_comments: int,
        output_dir: Path
    ) -> Dict:
        """
        ä¼˜åŒ–ç‰ˆæ•°æ®é‡‡é›†

        ç­–ç•¥ï¼š
        1. ä¸€æ¬¡æ€§æ„å»ºè§†é¢‘æ± 
        2. è§†é¢‘æ± éšæœºåˆ†é…åˆ°å„å­£åº¦
        3. æ¯è§†é¢‘é‡‡é›†100æ¡è¯„è®ºï¼ˆvs 30æ¡ï¼‰
        4. ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤æœç´¢

        Args:
            start_date: èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            total_comments: ç›®æ ‡æ€»è¯„è®ºæ•°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            é‡‡é›†ç»“æœç»Ÿè®¡
        """
        output_dir.mkdir(parents=True, exist_ok=True)

        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')

        # ç”Ÿæˆå­£åº¦åˆ—è¡¨
        quarters = self._generate_quarters(start_dt, end_dt)
        comments_per_quarter = total_comments // len(quarters)

        print("\n" + "="*80)
        print(" ğŸš€ é…é¢ä¼˜åŒ–é‡‡é›† - èŠ‚çœ87%é…é¢ï¼")
        print("="*80)
        print(f" æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        print(f" ç›®æ ‡è¯„è®º: {total_comments:,} æ¡")
        print(f" å­£åº¦æ•°é‡: {len(quarters)}")
        print(f" æ¯å­£åº¦: ~{comments_per_quarter:,} æ¡")
        print(f" ç­–ç•¥: ä¸€æ¬¡æœç´¢ â†’ è§†é¢‘æ±  â†’ å¤šæ¬¡ä½¿ç”¨")
        print("="*80)

        # è®¡ç®—éœ€è¦çš„è§†é¢‘æ€»æ•°
        comments_per_video = 100  # ä¼˜åŒ–ï¼šå¢åŠ åˆ°100
        total_videos_needed = (total_comments // comments_per_video) * 1.3  # å¤š30%å¤‡ç”¨
        total_videos_needed = int(total_videos_needed)

        # æ„å»ºè§†é¢‘æ± ï¼ˆåªæœç´¢ä¸€æ¬¡ï¼ï¼‰
        cache_file = output_dir / 'video_pool_cache.json'
        video_pool = self.build_video_pool(
            start_dt, end_dt, total_videos_needed, cache_file
        )

        if len(video_pool) < total_videos_needed * 0.5:
            print(f"\nâš ï¸ è­¦å‘Š: è§†é¢‘æ± ä¸è¶³ï¼Œå¯èƒ½æ— æ³•è¾¾åˆ°ç›®æ ‡")

        # å°†è§†é¢‘æ± éšæœºåˆ†é…åˆ°å„å­£åº¦
        videos_per_quarter = len(video_pool) // len(quarters)
        print(f"\nğŸ“¦ è§†é¢‘æ± åˆ†é…: æ¯å­£åº¦çº¦ {videos_per_quarter} ä¸ªè§†é¢‘")

        all_comments = []
        all_quarter_stats = []

        for idx, quarter_info in enumerate(quarters, 1):
            print(f"\nè¿›åº¦: [{idx}/{len(quarters)}] å­£åº¦")

            # ä»è§†é¢‘æ± åˆ†é…è§†é¢‘
            start_idx = idx * videos_per_quarter - videos_per_quarter
            end_idx = min(idx * videos_per_quarter, len(video_pool))
            quarter_videos = video_pool[start_idx:end_idx]

            try:
                comments, quarter_stats = self.collect_with_pool(
                    quarter_info['key'],
                    quarter_videos,
                    comments_per_quarter,
                    comments_per_video=comments_per_video
                )

                all_comments.extend(comments)
                all_quarter_stats.append(quarter_stats)

                # ä¿å­˜æ£€æŸ¥ç‚¹
                self._save_checkpoint(output_dir, comments, quarter_stats)

                # æ˜¾ç¤ºç´¯è®¡è¿›åº¦
                self._print_progress(all_quarter_stats, total_comments)

            except Exception as e:
                print(f"\nâŒ å­£åº¦ {quarter_info['key']} å¤±è´¥: {e}")
                continue

        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_result = self._save_final_results(
            output_dir, all_comments, all_quarter_stats
        )

        # æ‰“å°é…é¢ä½¿ç”¨æŠ¥å‘Š
        self._print_quota_report()

        return final_result

    def _generate_quarters(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """ç”Ÿæˆå­£åº¦åˆ—è¡¨"""
        quarters = []
        current = start_date

        while current <= end_date:
            year = current.year
            month = current.month
            quarter = (month - 1) // 3 + 1

            q_start = datetime(year, (quarter-1)*3 + 1, 1)
            if quarter < 4:
                q_end = datetime(year, quarter*3 + 1, 1) - timedelta(days=1)
            else:
                q_end = datetime(year, 12, 31)

            q_start = max(q_start, start_date)
            q_end = min(q_end, end_date)

            quarters.append({
                'key': f"{year}Q{quarter}",
                'start': q_start,
                'end': q_end
            })

            if quarter < 4:
                current = datetime(year, quarter*3 + 1, 1)
            else:
                current = datetime(year + 1, 1, 1)

        return quarters

    def _save_checkpoint(self, output_dir: Path, comments: List, quarter_stats: Dict):
        """ä¿å­˜å­£åº¦æ£€æŸ¥ç‚¹"""
        checkpoint_file = output_dir / f"checkpoint_{quarter_stats['quarter']}.json"
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump({
                'comments': comments,
                'stats': quarter_stats,
                'timestamp': datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)

    def _print_progress(self, all_stats: List[Dict], total_target: int):
        """æ‰“å°ç´¯è®¡è¿›åº¦"""
        total_collected = sum(s['collected_comments'] for s in all_stats)
        total_ai = sum(s['ai_comments'] for s in all_stats)
        overall_ai_ratio = (total_ai / total_collected * 100) if total_collected > 0 else 0

        print(f"\nğŸ“Š ç´¯è®¡è¿›åº¦:")
        print(f"   å·²é‡‡é›†: {total_collected:,} / {total_target:,} ({total_collected/total_target*100:.1f}%)")
        print(f"   AIå æ¯”: {overall_ai_ratio:.1f}%")
        print(f"   é…é¢ä½¿ç”¨: {self.stats['quota_used']:,} units")

    def _print_quota_report(self):
        """æ‰“å°é…é¢ä½¿ç”¨æŠ¥å‘Š"""
        print("\n" + "="*80)
        print(" ğŸ“Š APIé…é¢ä½¿ç”¨æŠ¥å‘Š")
        print("="*80)
        print(f"\nAPIè°ƒç”¨ç»Ÿè®¡:")
        print(f"  search.list: {self.stats['total_api_calls']['search']} æ¬¡ Ã— 100 = "
              f"{self.stats['total_api_calls']['search'] * 100:,} units")
        print(f"  videos.list: {self.stats['total_api_calls']['videos']} æ¬¡ Ã— 1 = "
              f"{self.stats['total_api_calls']['videos']:,} units")
        print(f"  commentThreads.list: {self.stats['total_api_calls']['comments']} æ¬¡ Ã— 1 = "
              f"{self.stats['total_api_calls']['comments']:,} units")
        print(f"\næ€»é…é¢æ¶ˆè€—: {self.stats['quota_used']:,} units")
        print(f"é…é¢åˆ©ç”¨ç‡: {self.stats['quota_used'] / 10000 * 100:.1f}% (æ—¥é™é¢10,000)")

        # å¯¹æ¯”åŸå§‹æ–¹æ³•
        original_quota = self.stats['quarters_completed'] * 2200  # ä¼°ç®—
        savings = original_quota - self.stats['quota_used']
        savings_pct = savings / original_quota * 100 if original_quota > 0 else 0
        print(f"\nğŸ’° é…é¢èŠ‚çœ:")
        print(f"  åŸå§‹æ–¹æ³•é¢„ä¼°: {original_quota:,} units")
        print(f"  ä¼˜åŒ–åå®é™…: {self.stats['quota_used']:,} units")
        print(f"  èŠ‚çœ: {savings:,} units ({savings_pct:.1f}%)")

    def _save_final_results(
        self, output_dir: Path, comments: List, quarter_stats: List[Dict]
    ) -> Dict:
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜æ‰€æœ‰è¯„è®º
        comments_file = output_dir / f'comments_optimized_{timestamp}.json'
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(comments, f, ensure_ascii=False, indent=2)

        # åˆ†ç¦»AIå’ŒéAI
        ai_comments = [c for c in comments if c.get('video_type') == 'ai_generated']
        non_ai_comments = [c for c in comments if c.get('video_type') == 'non_ai']

        ai_file = output_dir / f'comments_ai_{timestamp}.json'
        non_ai_file = output_dir / f'comments_non_ai_{timestamp}.json'

        with open(ai_file, 'w', encoding='utf-8') as f:
            json.dump(ai_comments, f, ensure_ascii=False, indent=2)
        with open(non_ai_file, 'w', encoding='utf-8') as f:
            json.dump(non_ai_comments, f, ensure_ascii=False, indent=2)

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'collection_timestamp': datetime.now().isoformat(),
            'method': 'quota_optimized',
            'total_comments': len(comments),
            'ai_comments': len(ai_comments),
            'non_ai_comments': len(non_ai_comments),
            'overall_ai_ratio': len(ai_comments) / len(comments) if comments else 0,
            'quarter_stats': quarter_stats,
            'api_stats': self.stats,
            'files': {
                'all_comments': str(comments_file),
                'ai_comments': str(ai_file),
                'non_ai_comments': str(non_ai_file)
            }
        }

        metadata_file = output_dir / f'metadata_optimized_{timestamp}.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ æ–‡ä»¶å·²ä¿å­˜:")
        print(f"   {comments_file}")
        print(f"   {ai_file}")
        print(f"   {non_ai_file}")
        print(f"   {metadata_file}")

        return metadata


def main():
    parser = argparse.ArgumentParser(
        description='é…é¢ä¼˜åŒ–é‡‡é›†å™¨ - èŠ‚çœ87%é…é¢'
    )
    parser.add_argument('--total', type=int, default=100000,
                       help='ç›®æ ‡æ€»è¯„è®ºæ•° (é»˜è®¤ 100,000)')
    parser.add_argument('--start-date', type=str, default='2022-01-01',
                       help='èµ·å§‹æ—¥æœŸ YYYY-MM-DD')
    parser.add_argument('--end-date', type=str, default='2025-10-31',
                       help='ç»“æŸæ—¥æœŸ YYYY-MM-DD')
    parser.add_argument('--output-dir', type=str, default='data/raw',
                       help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # æ£€æŸ¥ API key
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("\nâŒ é”™è¯¯: æœªè®¾ç½® YOUTUBE_API_KEY ç¯å¢ƒå˜é‡")
        print("\nè¯·å…ˆè®¾ç½® YouTube API å¯†é’¥:")
        print("  export YOUTUBE_API_KEY=your_api_key_here")
        return 1

    # åˆå§‹åŒ–é‡‡é›†å™¨
    try:
        collector = QuotaOptimizedCollector(api_key=api_key)
        print("\nâœ… YouTube API è¿æ¥æˆåŠŸ")
        print("âœ… AIæ£€æµ‹å™¨å·²åŠ è½½")
        print("âœ… é…é¢ä¼˜åŒ–ç­–ç•¥å·²å¯ç”¨")
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1

    # å¼€å§‹é‡‡é›†
    try:
        result = collector.collect_all_optimized(
            start_date=args.start_date,
            end_date=args.end_date,
            total_comments=args.total,
            output_dir=Path(args.output_dir)
        )
        print("\nğŸ‰ é‡‡é›†å®Œæˆï¼")
        return 0
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ é‡‡é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
